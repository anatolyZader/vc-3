// pgVectorService.js
//
// POSTGRESQL VECTOR STORE (Nov 2025)
// ====================================
// Replaces Pinecone with PostgreSQL + pgvector extension
// Uses LangChain's PGVectorStore for vector similarity search
// 
// Benefits over Pinecone:
//   âœ… Native PostgreSQL integration (no external dependencies)
//   âœ… HNSW index support for fast similarity search
//   âœ… Full SQL query capabilities with vector filtering
//   âœ… Better cost control and data ownership
//   âœ… Consistent with existing database infrastructure
//
// Compatible API with PineconeService for easy migration

const {
  PGVectorStore,
  DistanceStrategy,
} = require("@langchain/community/vectorstores/pgvector");
const { PoolConfig } = require("pg");
const MetadataFlattener = require('../chunking/metadataFlattener');

class PGVectorService {
  constructor(options = {}) {
    this.postgresConfig = {
      host: process.env.PG_HOST || process.env.LOCAL_DATABASE_HOST || 'localhost',
      port: parseInt(process.env.PG_PORT || process.env.LOCAL_DATABASE_PORT) || 5433,
      user: process.env.PG_USER || process.env.LOCAL_DATABASE_USER || 'eventstorm_user',
      password: process.env.PG_PASSWORD || process.env.LOCAL_DATABASE_PASSWORD || 'local_dev_password',
      database: process.env.PG_DATABASE || process.env.LOCAL_DATABASE_NAME || 'eventstorm_db',
      ssl: process.env.NODE_ENV === 'production' ? { rejectUnauthorized: false } : false,
    };

    // Table configuration for vector storage
    this.tableConfig = {
      tableName: options.tableName || 'langchain_pg_embedding',
      collectionName: options.collectionName || 'default',
      collectionTableName: options.collectionTableName || 'langchain_pg_collection',
      columns: {
        idColumnName: 'uuid',
        vectorColumnName: 'embedding',
        contentColumnName: 'document',
        metadataColumnName: 'cmetadata',
      },
      distanceStrategy: 'cosine',
    };
    
    // Rate limiting for operations
    this.rateLimiter = options.rateLimiter || null;
    
    this.logger = {
      info: (msg, ...args) => console.log(`[PGVectorService] ${msg}`, ...args),
      warn: (msg, ...args) => console.warn(`[PGVectorService] ${msg}`, ...args),
      error: (msg, ...args) => console.error(`[PGVectorService] ${msg}`, ...args),
      debug: (msg, ...args) => {
        if (process.env.NODE_ENV === 'development') {
          console.log(`[PGVectorService:DEBUG] ${msg}`, ...args);
        }
      }
    };

    // Cache for vector stores by collection name
    this.vectorStoreCache = new Map();
  }

  /**
   * Create or get cached PGVectorStore instance
   */
  async createVectorStore(embeddings, collectionName = 'default') {
    const cacheKey = collectionName || 'default';
    
    if (this.vectorStoreCache.has(cacheKey)) {
      return this.vectorStoreCache.get(cacheKey);
    }

    const config = {
      postgresConnectionOptions: this.postgresConfig,
      tableName: this.tableConfig.tableName,
      collectionName: collectionName,
      collectionTableName: this.tableConfig.collectionTableName,
      columns: this.tableConfig.columns,
      distanceStrategy: this.tableConfig.distanceStrategy,
    };

    try {
      this.logger.debug(`Creating PGVectorStore for collection: ${collectionName}`);
      
      const vectorStore = await PGVectorStore.initialize(embeddings, config);
      
      // Cache the vector store instance
      this.vectorStoreCache.set(cacheKey, vectorStore);
      
      this.logger.debug(`PGVectorStore created and cached for collection: ${collectionName}`);
      
      return vectorStore;
    } catch (error) {
      this.logger.error(`Failed to create PGVectorStore for collection ${collectionName}:`, error.message);
      throw error;
    }
  }

  /**
   * Create HNSW index for fast similarity search
   */
  async createHnswIndex(embeddings, options = {}) {
    const {
      dimensions = 1536, // Default for OpenAI text-embedding-3-small
      m = 16,
      efConstruction = 64,
      collectionName = 'default'
    } = options;

    try {
      const vectorStore = await this.createVectorStore(embeddings, collectionName);
      
      this.logger.info(`Creating HNSW index for collection: ${collectionName}`, {
        dimensions,
        m,
        efConstruction
      });
      
      await vectorStore.createHnswIndex({
        dimensions,
        m,
        efConstruction
      });
      
      this.logger.info(`HNSW index created successfully for collection: ${collectionName}`);
      
      return { success: true };
    } catch (error) {
      this.logger.error('Failed to create HNSW index:', error.message);
      throw error;
    }
  }

  /**
   * Execute a function with rate limiting if available
   */
  async withRateLimit(operation) {
    if (this.rateLimiter) {
      return this.rateLimiter.schedule(operation);
    }
    return operation();
  }

  /**
   * Upsert documents to PostgreSQL with proper error handling and rate limiting
   * Compatible with PineconeService API
   */
  async upsertDocuments(documents, embeddings, options = {}) {
    const {
      namespace = 'default', // Map namespace to collection name
      ids = null,
      batchSize = 100,
      onProgress = null,
      githubOwner = null,
      repoName = null,
      verbose = false,
      logPrefix = '[PGVectorService]'
    } = options;

    if (!documents || documents.length === 0) {
      if (verbose) {
        console.log(`[${new Date().toISOString()}] âš ï¸ DATA-PREP: No documents to store in PostgreSQL`);
        console.log(`[${new Date().toISOString()}] ðŸŽ¯ This usually means no processable files were found in the repository`);
      }
      this.logger.warn('No documents provided for upsert');
      return { success: true, processed: 0 };
    }

    try {
      const collectionName = namespace || 'default';
      const vectorStore = await this.createVectorStore(embeddings, collectionName);
      
      this.logger.info(`Upserting ${documents.length} documents`, {
        collectionName,
        batchSize,
        hasCustomIds: !!ids,
        repo: repoName ? `${githubOwner}/${repoName}` : undefined
      });

      if (verbose) {
        console.log(`[${new Date().toISOString()}] ðŸ”‘ ID GENERATION: Using deterministic PostgreSQL UUIDs for precise retrieval`);
        console.log(`[${new Date().toISOString()}] âš¡ BATCHING: Processing in batches to optimize PostgreSQL performance`);
      }

      // Process metadata for PostgreSQL storage
      const trimmedDocuments = documents.map(doc => {
        const result = MetadataFlattener.processForUpsert(doc.metadata);
        
        // Log validation warnings
        if (!result.validation.valid) {
          this.logger.warn(`Metadata issues for ${doc.metadata?.source || 'unknown'}:`, result.validation.issues);
        }
        
        return {
          ...doc,
          metadata: result.metadata
        };
      });

      if (verbose) {
        console.log(`[${new Date().toISOString()}] ðŸ§¹ METADATA_PROCESSING: Processed ${trimmedDocuments.length} documents for PostgreSQL storage`);
      }

      // Process documents in batches
      if (trimmedDocuments.length > batchSize) {
        let processed = 0;
        
        for (let i = 0; i < trimmedDocuments.length; i += batchSize) {
          const batch = trimmedDocuments.slice(i, i + batchSize);
          const batchIds = ids ? ids.slice(i, i + batchSize) : undefined;

          await this.withRateLimit(async () => {
            await vectorStore.addDocuments(batch, { ids: batchIds });
          });

          processed += batch.length;
          
          if (onProgress) {
            onProgress(processed, trimmedDocuments.length);
          }
          
          this.logger.debug(`Processed batch ${Math.floor(i / batchSize) + 1}`, {
            processed,
            total: trimmedDocuments.length
          });
        }
      } else {
        await this.withRateLimit(async () => {
          await vectorStore.addDocuments(trimmedDocuments, { ids });
        });
      }

      if (verbose) {
        console.log(`[${new Date().toISOString()}] âœ… DATA-PREP: Successfully stored ${trimmedDocuments.length} chunks to PostgreSQL collection: ${collectionName}`);
      }

      this.logger.info(`Successfully upserted ${trimmedDocuments.length} documents`, { collectionName });
      
      return {
        success: true,
        processed: trimmedDocuments.length,
        namespace: collectionName,
        documentIds: ids
      };

    } catch (error) {
      if (verbose) {
        console.error(`[${new Date().toISOString()}] âŒ DATA-PREP: Error storing documents to PostgreSQL:`, error);
        console.error(`[${new Date().toISOString()}] ðŸ’¡ This may be due to database connection issues or invalid document format`);
      }
      this.logger.error('Failed to upsert documents:', error.message);
      throw error;
    }
  }

  /**
   * Query similar documents from PostgreSQL
   * Compatible with PineconeService API
   */
  async querySimilar(queryEmbedding, options = {}) {
    const {
      namespace = 'default',
      topK = 10,
      filter = null,
      includeMetadata = true,
      includeValues = false
    } = options;

    try {
      const collectionName = namespace || 'default';
      const vectorStore = await this.createVectorStore(
        { embedQuery: () => queryEmbedding }, // Dummy embeddings for query
        collectionName
      );

      this.logger.debug('Querying similar vectors', {
        collectionName,
        topK,
        hasFilter: !!filter
      });

      // Use PGVectorStore's similaritySearchVectorWithScore method
      const results = await vectorStore.similaritySearchVectorWithScore(
        queryEmbedding,
        topK,
        filter
      );

      // Transform results to match Pinecone API format
      const matches = results.map(([doc, score]) => ({
        id: doc.id,
        score: score,
        values: includeValues ? queryEmbedding : undefined, // PostgreSQL doesn't store original vectors by default
        metadata: includeMetadata ? doc.metadata : undefined,
        content: doc.pageContent
      }));

      this.logger.debug(`Query returned ${matches.length} matches`);
      
      return {
        matches,
        namespace: collectionName
      };

    } catch (error) {
      this.logger.error('Failed to query similar vectors:', error.message);
      throw error;
    }
  }

  /**
   * Delete vectors by IDs
   * Compatible with PineconeService API
   */
  async deleteVectors(ids, namespace = 'default') {
    try {
      const collectionName = namespace || 'default';
      const vectorStore = await this.createVectorStore(
        { embedQuery: () => [] }, // Dummy embeddings for deletion
        collectionName
      );
      
      this.logger.info(`Deleting ${ids.length} vectors`, { collectionName });
      
      await vectorStore.delete({ ids });
      
      this.logger.info(`Successfully deleted ${ids.length} vectors`);
      
      return { success: true, deleted: ids.length };

    } catch (error) {
      this.logger.error('Failed to delete vectors:', error.message);
      throw error;
    }
  }

  /**
   * Delete all vectors in a collection (namespace equivalent)
   * Compatible with PineconeService API
   */
  async deleteNamespace(namespace) {
    try {
      const collectionName = namespace || 'default';
      
      this.logger.info(`Deleting all vectors in collection: ${collectionName}`);
      
      // For PostgreSQL, we can delete by collection name
      const vectorStore = await this.createVectorStore(
        { embedQuery: () => [] }, // Dummy embeddings
        collectionName
      );

      // Delete all documents in the collection
      await vectorStore.delete({ filter: {} });
      
      this.logger.info(`Successfully deleted collection: ${collectionName}`);
      
      return { success: true, namespace: collectionName };

    } catch (error) {
      this.logger.error(`Failed to delete collection ${collectionName}:`, error.message);
      throw error;
    }
  }

  /**
   * Clean up old repository embeddings
   * Compatible with PineconeService API
   */
  async cleanupOldRepositoryEmbeddings(namespace, options = {}) {
    const { dryRun = false } = options;
    
    try {
      const collectionName = namespace || 'default';
      this.logger.info(`Cleanup for collection: ${collectionName} (PostgreSQL native cleanup)`);
      
      if (dryRun) {
        this.logger.info('Dry run mode - would delete collection for fresh indexing');
        return { success: true, deleted: 0, kept: 0, mode: 'dry_run' };
      }

      await this.deleteNamespace(collectionName);
      
      this.logger.info(`Successfully cleaned collection: ${collectionName} (ready for fresh indexing)`);
      return { success: true, deleted: 'all', kept: 0, mode: 'full_collection_reset' };

    } catch (error) {
      this.logger.error(`Failed to clean collection ${collectionName}:`, error.message);
      // Don't throw - might be first time processing this repo
      return { success: false, deleted: 0, kept: 0, error: error.message };
    }
  }

  /**
   * Delete vectors for specific files
   * Compatible with PineconeService API
   */
  async deleteVectorsForFiles(namespace, filePaths) {
    try {
      if (!filePaths || filePaths.length === 0) {
        this.logger.info('No files specified for deletion');
        return { success: true, deleted: 0 };
      }

      const collectionName = namespace || 'default';
      this.logger.info(`Deleting vectors for ${filePaths.length} files in collection: ${collectionName}`);
      
      const vectorStore = await this.createVectorStore(
        { embedQuery: () => [] }, // Dummy embeddings
        collectionName
      );

      let totalDeleted = 0;

      for (const filePath of filePaths) {
        // Delete documents with matching source metadata
        const deleteResult = await vectorStore.delete({
          filter: { source: filePath }
        });
        
        if (deleteResult.deleted) {
          totalDeleted += deleteResult.deleted;
        }
      }

      this.logger.info(`Successfully deleted ${totalDeleted} vectors for ${filePaths.length} files`);
      return { success: true, deleted: totalDeleted };

    } catch (error) {
      this.logger.error(`Failed to delete vectors for files:`, error.message);
      throw error;
    }
  }

  /**
   * List all collections (namespace equivalent)
   * Compatible with PineconeService API
   */
  async listNamespaces() {
    try {
      // For PostgreSQL, we need to query the collections table
      // This is a simplified implementation
      this.logger.debug('Listing collections from PostgreSQL');
      
      // Return cached collections for now
      const collections = Array.from(this.vectorStoreCache.keys());
      
      return collections.map(name => ({
        name,
        vectorCount: 0 // Would need custom query to get exact count
      }));

    } catch (error) {
      this.logger.error('Failed to list collections:', error.message);
      throw error;
    }
  }

  /**
   * Get collection statistics
   * Compatible with PineconeService API
   */
  async getNamespaceStats(namespace) {
    try {
      const collectionName = namespace || 'default';
      
      // This would need a custom query to PostgreSQL to get accurate stats
      this.logger.debug(`Getting stats for collection: ${collectionName}`);
      
      return {
        name: collectionName,
        vectorCount: 0 // Placeholder - would need custom implementation
      };

    } catch (error) {
      this.logger.error(`Failed to get stats for collection ${collectionName}:`, error.message);
      throw error;
    }
  }

  /**
   * Close database connections
   * Compatible with PineconeService API
   */
  async disconnect() {
    try {
      // Close all cached vector store connections
      for (const [collectionName, vectorStore] of this.vectorStoreCache.entries()) {
        if (vectorStore && typeof vectorStore.end === 'function') {
          await vectorStore.end();
        }
      }
      
      this.vectorStoreCache.clear();
      this.logger.info('Disconnected from PostgreSQL vector stores');
    } catch (error) {
      this.logger.error('Error during disconnect:', error.message);
    }
  }

  /**
   * Check if service is connected
   */
  isConnected() {
    return this.vectorStoreCache.size > 0;
  }

  /**
   * Get connection status
   */
  isConnectedToIndex() {
    return this.isConnected();
  }

  /**
   * Get configuration
   */
  getConfig() {
    return {
      postgres: this.postgresConfig,
      table: this.tableConfig
    };
  }

  /**
   * Sanitize identifier for use as collection name
   * Compatible with PineconeService API
   */
  static sanitizeId(id) {
    return String(id).replace(/[^a-zA-Z0-9_-]/g, '_').toLowerCase();
  }

  /**
   * Generate consistent document ID
   * Compatible with PineconeService API
   */
  static generateDocumentId(prefix, parts) {
    const sanitized = parts.map(part => 
      this.sanitizeId(part)
    );
    return `${prefix}_${sanitized.join('_')}`;
  }

  /**
   * Generate deterministic document IDs based on content hash and source
   * Compatible with PineconeService API
   */
  static generateRepositoryDocumentIds(documents, namespace, options = {}) {
    const crypto = require('crypto');
    const { v4: uuidv4 } = require('uuid');
    
    return documents.map((doc, index) => {
      const sourceFile = doc.metadata?.source || 'unknown';
      const sanitizedSource = this.sanitizeId(sourceFile.replace(/\//g, '_'));
      
      // Create stable content hash for deterministic ID generation
      const contentHash = crypto
        .createHash('sha256')
        .update(doc.pageContent || '')
        .digest('hex')
        .substring(0, 16);
      
      // Store chunk metadata for tracking
      if (doc.metadata) {
        doc.metadata.contentHash = contentHash;
        doc.metadata.chunkIndex = index;
        doc.metadata.version = Date.now();
        doc.metadata.collection = namespace;
      }
      
      // Use UUID for PostgreSQL compatibility while maintaining determinism
      return uuidv4();
    });
  }

  /**
   * Generate deterministic document IDs for user repository chunks
   * Compatible with PineconeService API
   */
  static generateUserRepositoryDocumentIds(documents, userId, repoId, options = {}) {
    return this.generateRepositoryDocumentIds(documents, `${userId}_${repoId}`, options);
  }

  /**
   * Create a PGVectorService instance from environment
   */
  static fromEnvironment(options = {}) {
    return new PGVectorService(options);
  }
}

module.exports = PGVectorService;