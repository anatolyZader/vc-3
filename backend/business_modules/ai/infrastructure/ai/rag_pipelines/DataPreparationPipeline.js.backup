// DataPreparationPipeline.js
"use strict";

const { GithubRepoLoader } = require("@langchain/community/document_loaders/web/github");
const { RecursiveCharacterTextSplitter } = require("@langchain/textsplitters");
const { PineconeStore } = require('@langchain/pinecone');
const { OpenAIEmbeddings } = require('@langchain/openai');
const SemanticPreprocessor = require('./SemanticPreprocessor');
const ASTCodeSplitter = require('./ASTCodeSplitter');
const path = require('path');
const fs = require('fs');


/*
 * This pipeline handles the heavy lifting operations that should be done once
 * per repository rather than on every query.
 */
class DataPreparationPipeline {
  constructor(options = {}) {
    // Store dependencies passed from the adapter
    this.embeddings = options.embeddings;
    this.pinecone = options.pinecone;
    this.eventBus = options.eventBus;
    this.pineconeLimiter = options.pineconeLimiter;
    
    // Store reference to CoreDocsIndexer for core documentation operations
    this.coreDocsIndexer = options.coreDocsIndexer;
    
    // Initialize semantic preprocessor
    this.semanticPreprocessor = new SemanticPreprocessor();
    
    // Initialize AST-based code splitter
    this.astCodeSplitter = new ASTCodeSplitter({
      maxChunkSize: options.maxChunkSize || 2000,
      includeComments: true,
      includeImports: true
    });
    
    // Load ubiquitous language dictionary
    this.ubiquitousLanguage = this.loadUbiquitousLanguage();
    
    console.log(`[${new Date().toISOString()}] DataPreparationPipeline initialized with semantic preprocessing, AST-based splitting, and ubiquitous language dictionary`);
  }

  /**
   * Load EventStorm.me ubiquitous language dictionary
   */
  loadUbiquitousLanguage() {
    try {
      const dictPath = path.join(__dirname, 'ubiqLangDict.json');
      const dictContent = fs.readFileSync(dictPath, 'utf8');
      const dictionary = JSON.parse(dictContent);
      console.log(`[${new Date().toISOString()}] ðŸ“š UBIQ-LANG: Loaded dictionary with ${Object.keys(dictionary.businessModules).length} business modules`);
      return dictionary;
    } catch (error) {
      console.warn(`[${new Date().toISOString()}] âš ï¸ UBIQ-LANG: Failed to load ubiquitous language dictionary:`, error.message);
      return null;
    }
  }

  /**
   * Enhance document with ubiquitous language context
   */
  enhanceWithUbiquitousLanguage(document) {
    if (!this.ubiquitousLanguage) {
      return document;
    }

    const content = document.pageContent.toLowerCase();
    const source = document.metadata.source || '';
    
    // Detect business module context
    const detectedModule = this.detectBusinessModule(content, source);
    
    // Add domain-specific metadata
    const enhancedMetadata = {
      ...document.metadata,
      ubiq_business_module: detectedModule,
      ubiq_bounded_context: this.getBoundedContext(detectedModule),
      ubiq_domain_events: this.getRelevantDomainEvents(detectedModule),
      ubiq_terminology: this.extractRelevantTerms(content)
    };

    // Add contextual annotation
    const contextualAnnotation = this.generateContextualAnnotation(detectedModule, content);
    const enhancedContent = contextualAnnotation + document.pageContent;

    console.log(`[${new Date().toISOString()}] ðŸ“š UBIQ-LANG: Enhanced document with module '${detectedModule}' context`);

    return {
      pageContent: enhancedContent,
      metadata: enhancedMetadata
    };
  }

  /**
   * Detect which business module this document belongs to
   */
  detectBusinessModule(content, source) {
    const modules = this.ubiquitousLanguage.businessModules;
    
    // Check source path first - more reliable than content analysis
    for (const [moduleName, moduleData] of Object.entries(modules)) {
      // Check for exact module name in path
      if (source.toLowerCase().includes(`/${moduleName}/`) || 
          source.toLowerCase().includes(`\\${moduleName}\\`) ||
          source.toLowerCase().includes(`_modules/${moduleName}/`) ||
          source.toLowerCase().includes(`modules\\${moduleName}\\`)) {
        return moduleName;
      }
      
      // Check for module-specific path patterns
      if (moduleName === 'auth' && (source.includes('aop_modules/auth') || source.includes('auth/'))) {
        return moduleName;
      }
      
      if (moduleName === 'git' && source.includes('/git/')) {
        return moduleName;
      }
      
      if (moduleName === 'api' && source.includes('/api/')) {
        return moduleName;
      }
      
      if (moduleName === 'docs' && source.includes('/docs/')) {
        return moduleName;
      }
    }
    
    // Fallback: Check content for module-specific terms with higher threshold
    let bestMatch = { module: 'unknown', score: 0 };
    
    for (const [moduleName, moduleData] of Object.entries(modules)) {
      const score = this.calculateModuleRelevanceScore(content, moduleData);
      if (score > bestMatch.score && score > 0.5) { // Higher threshold for content-based detection
        bestMatch = { module: moduleName, score };
      }
    }
    
    return bestMatch.module;
  }

  /**
   * Calculate relevance score for a business module
   */
  calculateModuleRelevanceScore(content, moduleData) {
    let score = 0;
    let totalTerms = 0;

    // Check entities
    if (moduleData.entities) {
      moduleData.entities.forEach(entity => {
        totalTerms++;
        if (content.includes(entity.name.toLowerCase())) {
          score += 0.3;
        }
        if (entity.behaviors) {
          entity.behaviors.forEach(behavior => {
            totalTerms++;
            if (content.includes(behavior.toLowerCase())) {
              score += 0.2;
            }
          });
        }
      });
    }

    // Check domain events
    if (moduleData.domainEvents) {
      moduleData.domainEvents.forEach(event => {
        totalTerms++;
        if (content.includes(event.name.toLowerCase().replace('event', ''))) {
          score += 0.25;
        }
      });
    }

    // Check value objects
    if (moduleData.valueObjects) {
      moduleData.valueObjects.forEach(vo => {
        totalTerms++;
        if (content.includes(vo.name.toLowerCase())) {
          score += 0.2;
        }
      });
    }

    return totalTerms > 0 ? score / totalTerms : 0;
  }

  /**
   * Get bounded context for a business module
   */
  getBoundedContext(moduleName) {
    const moduleData = this.ubiquitousLanguage.businessModules[moduleName];
    return moduleData ? moduleData.boundedContext : 'Unknown Context';
  }

  /**
   * Get relevant domain events for a business module
   */
  getRelevantDomainEvents(moduleName) {
    const events = this.ubiquitousLanguage.domainEvents[moduleName];
    return events ? events : [];
  }

  /**
   * Extract relevant business terms from content
   */
  extractRelevantTerms(content) {
    const relevantTerms = [];
    
    // Check business terms
    for (const [term, definition] of Object.entries(this.ubiquitousLanguage.businessTerms || {})) {
      if (content.includes(term.toLowerCase()) || content.includes(definition.name.toLowerCase())) {
        relevantTerms.push(term);
      }
    }
    
    // Check technical terms
    for (const [term, definition] of Object.entries(this.ubiquitousLanguage.technicalTerms || {})) {
      if (content.includes(term.toLowerCase()) || content.includes(definition.name.toLowerCase())) {
        relevantTerms.push(term);
      }
    }
    
    return relevantTerms;
  }

  /**
   * Generate contextual annotation based on detected module and content
   */
  generateContextualAnnotation(moduleName, content) {
    if (moduleName === 'unknown') {
      return '// UBIQUITOUS LANGUAGE CONTEXT: Unknown module\n';
    }

    const moduleData = this.ubiquitousLanguage.businessModules[moduleName];
    if (!moduleData) {
      return '// UBIQUITOUS LANGUAGE CONTEXT: Unknown module\n';
    }

    let annotation = `// UBIQUITOUS LANGUAGE CONTEXT: ${moduleData.name.toUpperCase()}\n`;
    annotation += `// BOUNDED CONTEXT: ${moduleData.boundedContext}\n`;
    annotation += `// DOMAIN ROLE: ${moduleData.role}\n`;
    
    if (moduleData.type) {
      annotation += `// MODULE TYPE: ${moduleData.type}\n`;
    }
    
    // Add relevant entities if found in content
    const relevantEntities = moduleData.entities?.filter(entity => 
      content.toLowerCase().includes(entity.name.toLowerCase())
    );
    if (relevantEntities && relevantEntities.length > 0) {
      annotation += `// RELEVANT ENTITIES: ${relevantEntities.map(e => e.name).join(', ')}\n`;
    }

    // Add relevant domain events if applicable
    const relevantEvents = this.ubiquitousLanguage.domainEvents[moduleName]?.filter(event =>
      content.toLowerCase().includes(event.toLowerCase().replace('event', ''))
    );
    if (relevantEvents && relevantEvents.length > 0) {
      annotation += `// DOMAIN EVENTS: ${relevantEvents.join(', ')}\n`;
    }

    annotation += '//\n';
    return annotation;
  }

  /**
   * Main entry point for processing a pushed repository
   */
  async processPushedRepo(userId, repoId, repoData) {
    console.log(`[${new Date().toISOString()}] ðŸ“¥ DATA-PREP: Processing repo for user ${userId}: ${repoId}`);
    console.log(`[${new Date().toISOString()}] ðŸ“¥ DATA-PREP: Received repoData structure:`, JSON.stringify(repoData, null, 2)); 
    
    console.log(`[${new Date().toISOString()}] ðŸŽ¯ STAGE EXPLANATION: Starting comprehensive repository processing pipeline`);
    console.log(`[${new Date().toISOString()}] ðŸŽ¯ This pipeline will: 1) Index core docs, 2) Clone repo, 3) Load files, 4) Apply domain knowledge, 5) Semantic analysis, 6) Intelligent chunking, 7) Vector storage`);
    
    // Emit starting status
    this.emitRagStatus('processing_started', {
      userId,
      repoId,
      timestamp: new Date().toISOString()
    });

    console.log(`[${new Date().toISOString()}] ðŸ”µ STAGE 1: CORE DOCUMENTATION INDEXING`);
    console.log(`[${new Date().toISOString()}] ðŸŽ¯ EXPLANATION: Before processing repository code, we first ensure our system-wide documentation (API specs, architecture docs, business module docs) is indexed and available for context`);
    
    // Index the core documentation first (API spec, markdown files)
    await this.indexCoreDocsToPinecone();

    try {
      console.log(`[${new Date().toISOString()}] ðŸ”µ STAGE 2: REPOSITORY VALIDATION & SETUP`);
      console.log(`[${new Date().toISOString()}] ðŸŽ¯ EXPLANATION: Validating repository data, extracting GitHub information, and checking for duplicate processing to avoid unnecessary work`);
      
      // Validate repoData structure
      if (!repoData || !repoData.url || !repoData.branch) {
        throw new Error(`Invalid repository data: ${JSON.stringify(repoData)}`);
      }

      const { url, branch } = repoData;

      // Extract GitHub owner and repo name from URL
      const urlParts = url.split('/');
      const githubOwner = urlParts[urlParts.length - 2];
      const repoName = urlParts[urlParts.length - 1].replace('.git', '');

      console.log(`[${new Date().toISOString()}] ðŸ“¥ DATA-PREP: Extracted GitHub owner: ${githubOwner}, repo name: ${repoName}`);

      console.log(`[${new Date().toISOString()}] ðŸ” DUPLICATE CHECK: Verifying if repository ${githubOwner}/${repoName} has already been processed to avoid redundant work`);
      
      // Check if the repository is already processed
      const existingRepo = await this.findExistingRepo(userId, repoId, githubOwner, repoName);
      if (existingRepo) {
        console.log(`[${new Date().toISOString()}] Repository already processed, skipping: ${repoId}`);
        this.emitRagStatus('processing_skipped', {
          userId,
          repoId,
          reason: 'Repository already processed',
          timestamp: new Date().toISOString()
        });
        return {
          success: true,
          message: 'Repository already processed',
          repoId,
          userId
        };
      }

      console.log(`[${new Date().toISOString()}] ðŸ”µ STAGE 3: REPOSITORY CLONING`);
      console.log(`[${new Date().toISOString()}] ðŸŽ¯ EXPLANATION: Cloning the repository to a temporary directory so we can access all source files for processing. This creates a local copy we can safely analyze without affecting the original repo`);

      // Clone the repository to a temporary location
      const tempDir = await this.cloneRepository(url, branch);

      console.log(`[${new Date().toISOString()}] ðŸ”µ STAGE 4: FILE LOADING & METADATA ENHANCEMENT`);
      console.log(`[${new Date().toISOString()}] ðŸŽ¯ EXPLANATION: Reading all files from the cloned repository, filtering out binary/unwanted files, and enhancing each document with metadata (user, repo info, timestamps, file types) for better tracking and retrieval`);

      // Load and process documents from the cloned repository
      const result = await this.loadAndProcessRepoDocuments(tempDir, userId, repoId, githubOwner, repoName);

      console.log(`[${new Date().toISOString()}] ðŸ”µ STAGE 5: CLEANUP & COMPLETION`);
      console.log(`[${new Date().toISOString()}] ðŸŽ¯ EXPLANATION: Removing the temporary cloned repository directory to free up disk space, since all necessary processing is complete and files are now stored as vector embeddings`);

      // Cleanup: remove the cloned repository files
      await this.cleanupTempDir(tempDir);

      console.log(`[${new Date().toISOString()}] ðŸŽ‰ PIPELINE COMPLETE: Successfully processed repository with domain knowledge, semantic analysis, and intelligent chunking`);
      console.log(`[${new Date().toISOString()}] ðŸ“Š FINAL RESULT: Repository ${githubOwner}/${repoName} is now ready for intelligent code queries and RAG-enhanced responses`);

      return result;
    } catch (error) {
      console.error(`[${new Date().toISOString()}] âŒ Error processing repository ${repoId}:`, error.message);
      
      // Emit error status
      this.emitRagStatus('processing_error', {
        userId,
        repoId,
        error: error.message,
        phase: 'repository_processing',
        processedAt: new Date().toISOString()
      });
      
      return {
        success: false,
        error: `Repository processing failed: ${error.message}`,
        userId: userId,
        repoId: repoId,
        processedAt: new Date().toISOString()
      };
    }
  }

  /**
   * Index core documentation (API spec and markdown files) into Pinecone
   * Delegates to CoreDocsIndexer for consistent processing
   */
  async indexCoreDocsToPinecone() {
    console.log(`[${new Date().toISOString()}] ðŸ”µ [RAG-INDEX] Delegating core docs indexing to CoreDocsIndexer...`);
    console.log(`[${new Date().toISOString()}] ðŸŽ¯ CORE DOCS EXPLANATION: Processing system-wide documentation that provides context for all repositories`);
    console.log(`[${new Date().toISOString()}] ðŸŽ¯ This includes: API specifications (endpoints, schemas, responses), architectural documentation, business module descriptions, and root documentation. This foundational knowledge helps the AI understand the overall system design and business context when analyzing specific repository code`);
    
    if (!this.coreDocsIndexer) {
      console.warn(`[${new Date().toISOString()}] âš ï¸ [RAG-INDEX] CoreDocsIndexer not available, skipping core docs indexing.`);
      return;
    }
    
    try {
      await this.coreDocsIndexer.indexCoreDocsToPinecone('core-docs', false);
      console.log(`[${new Date().toISOString()}] âœ… [RAG-INDEX] Core docs indexing completed successfully.`);
      console.log(`[${new Date().toISOString()}] ðŸŽ¯ RESULT: System documentation is now available as context for intelligent code analysis and responses`);
    } catch (error) {
      console.error(`[${new Date().toISOString()}] âŒ [RAG-INDEX] Error during core docs indexing:`, error.message);
      throw error;
    }
  }

  /**
   * Clone repository to temporary directory
   */
  async cloneRepository(url, branch) {
    const fs = require('fs').promises;
    const path = require('path');
    const { exec } = require('child_process');
    const { promisify } = require('util');
    const execAsync = promisify(exec);
    
    console.log(`[${new Date().toISOString()}] ðŸŽ¯ CLONING EXPLANATION: Creating isolated temporary workspace for safe repository analysis`);
    console.log(`[${new Date().toISOString()}] ðŸŽ¯ We use 'git clone --depth 1' for efficiency (only latest commit) and create a unique temporary directory to avoid conflicts with concurrent processing`);
    
    // Create temp directory
    const tempDir = path.join(__dirname, '../../../../../../temp', `repo_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`);
    
    try {
      await fs.mkdir(tempDir, { recursive: true });
      console.log(`[${new Date().toISOString()}] ï¿½ TEMP WORKSPACE: Created isolated directory: ${tempDir}`);
      
      // Clone the repository
      const cloneCommand = `git clone --depth 1 --branch ${branch} ${url} ${tempDir}`;
      console.log(`[${new Date().toISOString()}] ðŸ“¥ GIT OPERATION: Executing shallow clone - ${cloneCommand}`);
      console.log(`[${new Date().toISOString()}] ðŸŽ¯ This downloads only the latest commit from branch '${branch}' to minimize transfer time and storage`);
      
      await execAsync(cloneCommand, { timeout: 60000 }); // 60 second timeout
      console.log(`[${new Date().toISOString()}] âœ… CLONE SUCCESS: Repository cloned successfully to temporary workspace`);
      console.log(`[${new Date().toISOString()}] ðŸ“‚ Ready to analyze source files from ${url}`);
      
      return tempDir;
    } catch (error) {
      console.error(`[${new Date().toISOString()}] âŒ DATA-PREP: Error cloning repository:`, error.message);
      
      // Cleanup on failure
      try {
        await fs.rmdir(tempDir, { recursive: true });
      } catch (cleanupError) {
        console.warn(`[${new Date().toISOString()}] âš ï¸ DATA-PREP: Could not cleanup temp directory:`, cleanupError.message);
      }
      
      throw new Error(`Failed to clone repository: ${error.message}`);
    }
  }

  /**
   * Load and process documents from cloned repository
   */
  async loadAndProcessRepoDocuments(tempDir, userId, repoId, githubOwner, repoName) {
    const fs = require('fs').promises;
    const path = require('path');
    
    try {
      console.log(`[${new Date().toISOString()}] ðŸ“¥ DATA-PREP: Processing documents from ${tempDir}`);
      
      // Use GitHub repo loader to process the cloned repository
      const loader = new GithubRepoLoader(tempDir, {
        branch: "main", // This will be ignored since we're loading from local directory
        recursive: true,
        unknown: "warn",
        maxConcurrency: 5, // Limit concurrent file processing
      });

      console.log(`[${new Date().toISOString()}] ðŸ“¥ DATA-PREP: Loading documents...`);
      const documents = await loader.load();
      
      if (documents.length === 0) {
        console.warn(`[${new Date().toISOString()}] âš ï¸ DATA-PREP: No documents found in repository`);
        return {
          success: true,
          message: 'No documents found to process',
          documentsProcessed: 0,
          userId,
          repoId
        };
      }

      console.log(`[${new Date().toISOString()}] ðŸ“¥ DATA-PREP: Loaded ${documents.length} documents`);

      // Enhance metadata for better tracking
      const enhancedDocuments = documents.map(doc => ({
        ...doc,
        metadata: {
          ...doc.metadata,
          userId,
          repoId,
          githubOwner,
          repoName,
          processedAt: new Date().toISOString(),
          fileType: this.getFileType(doc.metadata.source || '')
        }
      }));

      console.log(`[${new Date().toISOString()}] ðŸ”µ STAGE 4A: UBIQUITOUS LANGUAGE ENHANCEMENT`);
      console.log(`[${new Date().toISOString()}] ðŸŽ¯ EXPLANATION: Applying domain-driven design principles by detecting business contexts and modules within code`);
      console.log(`[${new Date().toISOString()}] ðŸŽ¯ This stage analyzes file paths and content to identify EventStorm business modules (auth, chat, wiki, etc.) and adds domain-specific terminology and context annotations to help the AI understand business intent, not just code syntax`);

      // Step 1: Enhance with ubiquitous language context (first step in RAG pipeline)
      console.log(`[${new Date().toISOString()}] ðŸ“š DATA-PREP: Applying ubiquitous language enhancement...`);
      const ubiqLanguageEnhancedDocs = [];
      for (const doc of enhancedDocuments) {
        try {
          const ubiqEnhancedDoc = this.enhanceWithUbiquitousLanguage(doc);
          ubiqLanguageEnhancedDocs.push(ubiqEnhancedDoc);
        } catch (error) {
          console.warn(`[${new Date().toISOString()}] âš ï¸ DATA-PREP: Failed to enhance ${doc.metadata.source} with ubiquitous language: ${error.message}`);
          // Fall back to original document if enhancement fails
          ubiqLanguageEnhancedDocs.push(doc);
        }
      }
      
      console.log(`[${new Date().toISOString()}] ðŸ“š DATA-PREP: Ubiquitous language enhancement completed for ${ubiqLanguageEnhancedDocs.length} documents`);

      console.log(`[${new Date().toISOString()}] ðŸ”µ STAGE 4B: SEMANTIC PREPROCESSING`);
      console.log(`[${new Date().toISOString()}] ðŸŽ¯ EXPLANATION: Analyzing code structure and architecture patterns to identify technical roles and relationships`);
      console.log(`[${new Date().toISOString()}] ðŸŽ¯ This stage examines each file to determine if it's a controller, service, model, utility, etc., identifies entry points, measures complexity, and maps architectural layers (presentation, business, data). This helps the AI understand not just WHAT the code does, but WHERE it fits in the overall system architecture`);

      // Step 2: Apply semantic preprocessing to enhance chunks with technical context
      console.log(`[${new Date().toISOString()}] ðŸ§  DATA-PREP: Applying semantic preprocessing...`);
      const semanticallyEnhancedDocs = [];
      for (const doc of ubiqLanguageEnhancedDocs) {
        try {
          const enhancedDoc = await this.semanticPreprocessor.preprocessChunk(doc);
          semanticallyEnhancedDocs.push(enhancedDoc);
        } catch (error) {
          console.warn(`[${new Date().toISOString()}] âš ï¸ DATA-PREP: Failed to preprocess ${doc.metadata.source}: ${error.message}`);
          // Fall back to original document if preprocessing fails
          semanticallyEnhancedDocs.push(doc);
        }
      }
      
      console.log(`[${new Date().toISOString()}] ðŸ§  DATA-PREP: Semantic preprocessing completed for ${semanticallyEnhancedDocs.length} documents`);

      // Log processing pipeline summary
      this.logProcessingPipelineSummary(ubiqLanguageEnhancedDocs, semanticallyEnhancedDocs);

      console.log(`[${new Date().toISOString()}] ðŸ”µ STAGE 4C: INTELLIGENT DOCUMENT CHUNKING`);
      console.log(`[${new Date().toISOString()}] ðŸŽ¯ EXPLANATION: Breaking documents into optimal chunks for vector storage and retrieval`);
      console.log(`[${new Date().toISOString()}] ðŸŽ¯ Code files use AST (Abstract Syntax Tree) parsing to split along semantic boundaries (functions, classes, methods) preserving logical relationships and context. Documentation files use traditional text splitting with overlap for continuity. This ensures each chunk is a meaningful, self-contained unit that the AI can reason about effectively`);

      // Step 3: Apply AST-based splitting for code files, regular splitting for others
      console.log(`[${new Date().toISOString()}] ðŸ“¥ DATA-PREP: Applying intelligent splitting (AST for code, regular for docs)...`);
      const splitDocs = await this.intelligentSplitDocuments(semanticallyEnhancedDocs);
      
      console.log(`[${new Date().toISOString()}] ðŸ“¥ DATA-PREP: Split into ${splitDocs.length} chunks`);

      console.log(`[${new Date().toISOString()}] ðŸ”µ STAGE 4D: VECTOR EMBEDDING & STORAGE`);
      console.log(`[${new Date().toISOString()}] ðŸŽ¯ EXPLANATION: Converting text chunks into high-dimensional vectors and storing in Pinecone for semantic search`);
      console.log(`[${new Date().toISOString()}] ðŸŽ¯ Each chunk is processed by OpenAI's embedding model to create a 1536-dimensional vector that captures semantic meaning. These vectors are stored in Pinecone with unique IDs and metadata, organized by user namespace for isolation. This enables lightning-fast similarity search during RAG queries`);

      // Log chunk breakdown for repository documents
      console.log(`[${new Date().toISOString()}] ðŸ“‹ [DATA-PREP] SEMANTICALLY ENHANCED + AST-SPLIT REPOSITORY CHUNK BREAKDOWN:`);
      splitDocs.forEach((doc, index) => {
        const preview = doc.pageContent.substring(0, 100).replace(/\n/g, ' ').trim();
        const semanticInfo = doc.metadata.enhanced ? 
          `${doc.metadata.semantic_role}|${doc.metadata.layer}|${doc.metadata.eventstorm_module}` : 
          'not-enhanced';
        const astInfo = doc.metadata.chunk_type || 'regular';
        const astDetails = doc.metadata.semantic_unit ? 
          `${doc.metadata.semantic_unit}(${doc.metadata.function_name})` : 
          'n/a';
        
        console.log(`[${new Date().toISOString()}] ðŸ“„ [REPO-CHUNK ${index + 1}/${splitDocs.length}] ${doc.metadata.source} (${doc.pageContent.length} chars)`);
        console.log(`[${new Date().toISOString()}] ðŸ“ Preview: ${preview}${doc.pageContent.length > 100 ? '...' : ''}`);
        console.log(`[${new Date().toISOString()}] ðŸ·ï¸  FileType: ${doc.metadata.fileType}, Repo: ${doc.metadata.repoName}`);
        console.log(`[${new Date().toISOString()}] ðŸ§  Semantic: ${semanticInfo}, EntryPoint: ${doc.metadata.is_entrypoint || false}, Complexity: ${doc.metadata.complexity || 'unknown'}`);
        console.log(`[${new Date().toISOString()}] ðŸŒ³ AST: ${astInfo}, Unit: ${astDetails}, Lines: ${doc.metadata.start_line || '?'}-${doc.metadata.end_line || '?'}`);
        console.log(`[${new Date().toISOString()}] ${'â”€'.repeat(80)}`);
      });

      // Generate unique IDs for each chunk
      const documentIds = splitDocs.map((doc, index) => {
        const sourceFile = doc.metadata.source || 'unknown';
        const sanitizedSource = this.sanitizeId(sourceFile.replace(/\//g, '_'));
        return `${userId}_${repoId}_${sanitizedSource}_chunk_${index}`;
      });

      console.log(`[${new Date().toISOString()}] ðŸš€ PINECONE STORAGE: Storing ${splitDocs.length} vector embeddings with unique IDs in user-specific namespace '${userId}'`);

      // Store in Pinecone with user-specific namespace
      if (this.pinecone) {
        try {
          const index = this.pinecone.Index(process.env.PINECONE_INDEX_NAME || 'eventstorm-index');
          const vectorStore = new PineconeStore(this.embeddings, {
            pineconeIndex: index,
            namespace: userId // User-specific namespace
          });

          // Use bottleneck limiter for Pinecone operations
          if (this.pineconeLimiter) {
            await this.pineconeLimiter.schedule(async () => {
              await vectorStore.addDocuments(splitDocs, { ids: documentIds });
            });
          } else {
            await vectorStore.addDocuments(splitDocs, { ids: documentIds });
          }

          console.log(`[${new Date().toISOString()}] âœ… DATA-PREP: Successfully indexed ${splitDocs.length} document chunks to Pinecone`);

          console.log(`[${new Date().toISOString()}] ðŸŽ‰ REPOSITORY PROCESSING COMPLETE!`);
          console.log(`[${new Date().toISOString()}] ðŸŽ¯ FINAL STATUS: Repository ${githubOwner}/${repoName} has been fully processed through our advanced RAG pipeline:`);
          console.log(`[${new Date().toISOString()}] ðŸ“ˆ - ${documents.length} source files analyzed and enhanced with domain knowledge`);
          console.log(`[${new Date().toISOString()}] ðŸ§  - Semantic structure and architectural roles identified`);
          console.log(`[${new Date().toISOString()}] ðŸ“Š - ${splitDocs.length} intelligent chunks created using AST parsing for code`);
          console.log(`[${new Date().toISOString()}] ðŸ” - All chunks embedded as vectors and stored in Pinecone for fast retrieval`);
          console.log(`[${new Date().toISOString()}] ðŸš€ - Ready for intelligent code queries, architectural questions, and context-aware responses!`);

          this.emitRagStatus('processing_completed', {
            userId,
            repoId,
            documentsProcessed: documents.length,
            chunksGenerated: splitDocs.length,
            githubOwner,
            repoName,
            timestamp: new Date().toISOString()
          });

          return {
            success: true,
            message: 'Repository processed successfully',
            documentsProcessed: documents.length,
            chunksGenerated: splitDocs.length,
            userId,
            repoId,
            githubOwner,
            repoName,
            processedAt: new Date().toISOString()
          };

        } catch (error) {
          console.error(`[${new Date().toISOString()}] âŒ DATA-PREP: Error storing in Pinecone:`, error.message);
          throw error;
        }
      } else {
        throw new Error('Pinecone client not available');
      }

    } catch (error) {
      console.error(`[${new Date().toISOString()}] âŒ DATA-PREP: Error processing repository documents:`, error.message);
      throw error;
    }
  }

  /**
   * Clean up temporary directory
   */
  async cleanupTempDir(tempDir) {
    const fs = require('fs').promises;
    
    console.log(`[${new Date().toISOString()}] ðŸ§¹ CLEANUP EXPLANATION: Removing temporary repository clone to free disk space`);
    console.log(`[${new Date().toISOString()}] ðŸŽ¯ Since all source files have been processed and stored as vector embeddings in Pinecone, the local copy is no longer needed. This prevents disk space accumulation from multiple repository processings`);
    
    try {
      await fs.rmdir(tempDir, { recursive: true });
      console.log(`[${new Date().toISOString()}] âœ… CLEANUP SUCCESS: Removed temporary directory: ${tempDir}`);
      console.log(`[${new Date().toISOString()}] ðŸ’¾ Disk space preserved - only vector embeddings retained for fast retrieval`);
    } catch (error) {
      console.warn(`[${new Date().toISOString()}] âš ï¸ CLEANUP WARNING: Could not remove temp directory ${tempDir}:`, error.message);
      console.log(`[${new Date().toISOString()}] ðŸŽ¯ This may require manual cleanup, but doesn't affect the processing success`);
    }
  }

  /**
   * Check if repository is already processed
   */
  async findExistingRepo(userId, repoId, githubOwner, repoName) {
    console.log(`[${new Date().toISOString()}] ðŸ” DUPLICATE CHECK EXPLANATION: Querying for existing repository data to prevent unnecessary reprocessing`);
    console.log(`[${new Date().toISOString()}] ðŸŽ¯ This optimization saves time and resources by checking if user ${userId} has already processed repo ${githubOwner}/${repoName}. We search in Pinecone vector metadata or database records to verify if vector embeddings already exist for this repository`);
    
    console.log(`[${new Date().toISOString()}] ðŸ“¥ DATA-PREP: Checking for existing repo: ${githubOwner}/${repoName}`);
    
    // For now, we'll assume repositories are not duplicate processed
    // This could be enhanced to check Pinecone metadata or a database
    console.log(`[${new Date().toISOString()}] ï¿½ NOTE: Duplicate checking not yet fully implemented - defaulting to process (safe mode)`);
    console.log(`[${new Date().toISOString()}] ðŸŽ¯ Future implementation will query Pinecone metadata with filters: userId=${userId}, githubOwner=${githubOwner}, repoName=${repoName}`);
    
    // TODO: Implement actual duplicate detection
    // This could query Pinecone metadata or check a database
    return false;
  }

  // Helper methods from original adapter

  /**
   * Helper method to determine file type from file path
   */
  getFileType(filePath) {
    const extension = filePath.split('.').pop().toLowerCase();
    const codeExtensions = {
      js: 'JavaScript',
      jsx: 'React',
      ts: 'TypeScript',
      tsx: 'React TypeScript',
      py: 'Python',
      java: 'Java',
      rb: 'Ruby',
      php: 'PHP',
      c: 'C',
      cpp: 'C++',
      cs: 'C#',
      go: 'Go',
      rs: 'Rust',
      swift: 'Swift',
      kt: 'Kotlin',
      html: 'HTML',
      css: 'CSS',
      scss: 'SCSS',
      json: 'JSON',
      md: 'Markdown',
      sql: 'SQL',
      sh: 'Shell',
      bat: 'Batch',
      ps1: 'PowerShell',
      yaml: 'YAML',
      yml: 'YAML',
      xml: 'XML'
    };

    return codeExtensions[extension] || 'Unknown';
  }

  /**
   * Helper method to sanitize document IDs
   */
  sanitizeId(input) {
    // Remove special characters and truncate if necessary
    return input.replace(/[^a-zA-Z0-9_-]/g, '_').substring(0, 50);
  }

  /**
   * Helper method to emit RAG status updates for monitoring
   */
  emitRagStatus(status, details = {}) {
    // Always log the status update
    console.log(`[${new Date().toISOString()}] ðŸ” RAG STATUS: ${status}`, 
      Object.keys(details).length > 0 ? JSON.stringify(details, null, 2) : '');
    
    // Try to emit to the event bus if available
    try {
      if (this.eventBus) {
        this.eventBus.emit('ragStatusUpdate', {
          component: 'DataPreparationPipeline',
          timestamp: new Date().toISOString(),
          status,
          ...details
        });
        return;
      }
      
      // Fallback to imported event bus if instance one isn't available
      const eventDispatcherPath = '../../../../../eventDispatcher';
      const { eventBus } = require(eventDispatcherPath);
      if (eventBus) {
        eventBus.emit('ragStatusUpdate', {
          component: 'DataPreparationPipeline',
          timestamp: new Date().toISOString(),
          status,
          ...details
        });
      }
    } catch (error) {
      console.warn(`[${new Date().toISOString()}] âš ï¸ Failed to emit RAG status update: ${error.message}`);
    }
  }

  /**
   * Apply intelligent splitting: AST-based for code, regular for documentation
   */
  async intelligentSplitDocuments(documents) {
    console.log(`[${new Date().toISOString()}] ðŸŽ¯ INTELLIGENT SPLITTING EXPLANATION: Using different strategies based on file type for optimal chunking`);
    console.log(`[${new Date().toISOString()}] ðŸŒ³ AST STRATEGY: Code files (.js, .ts, .py, etc.) are parsed into Abstract Syntax Trees to identify functions, classes, methods`);
    console.log(`[${new Date().toISOString()}] ðŸ“„ REGULAR STRATEGY: Documentation files (.md, .txt, etc.) use traditional text splitting with overlap for continuity`);
    console.log(`[${new Date().toISOString()}] ðŸŽ¯ This ensures each chunk represents a complete, logical unit that maintains context and meaning`);
    
    const allSplitDocs = [];
    const astStats = { attempted: 0, successful: 0, failed: 0, fallback: 0 };
    
    for (const doc of documents) {
      const source = doc.metadata.source || '';
      const isCodeFile = this.astCodeSplitter.shouldUseASTSplitting(doc.metadata);
      
      if (isCodeFile) {
        console.log(`[${new Date().toISOString()}] ðŸŒ³ CODE FILE DETECTED: ${source} - Will attempt AST-based semantic splitting`);
        astStats.attempted++;
        try {
          console.log(`[${new Date().toISOString()}] AST: Processing ${source}`);
          const astChunks = await this.astCodeSplitter.splitCodeDocument(doc);
          
          if (astChunks.length > 1 || (astChunks[0] && astChunks[0].metadata.chunk_type === 'ast_semantic')) {
            astStats.successful++;
            allSplitDocs.push(...astChunks);
            console.log(`[${new Date().toISOString()}] âœ… AST SUCCESS: Split ${source} into ${astChunks.length} semantic chunks (functions, classes, methods)`);
          } else {
            astStats.fallback++;
            // Fallback to regular splitting
            const regularChunks = await this.regularSplitDocument(doc);
            allSplitDocs.push(...regularChunks);
            console.log(`[${new Date().toISOString()}] ðŸ“„ AST FALLBACK: Used regular text splitting for ${source} (AST parsing didn't identify clear semantic boundaries)`);
          }
        } catch (error) {
          astStats.failed++;
          console.warn(`[${new Date().toISOString()}] âŒ AST FAILED: Could not parse ${source}, using regular splitting: ${error.message}`);
          const regularChunks = await this.regularSplitDocument(doc);
          allSplitDocs.push(...regularChunks);
        }
      } else {
        console.log(`[${new Date().toISOString()}] ðŸ“„ DOCUMENTATION FILE: ${source} - Using regular text splitting with overlap`);
        // Use regular splitting for non-code files
        const regularChunks = await this.regularSplitDocument(doc);
        allSplitDocs.push(...regularChunks);
      }
    }
    
    console.log(`[${new Date().toISOString()}] ðŸ“Š SPLITTING STRATEGY RESULTS:`);
    console.log(`[${new Date().toISOString()}] ðŸŒ³ AST attempted: ${astStats.attempted} code files`);
    console.log(`[${new Date().toISOString()}] âœ… AST successful: ${astStats.successful} files parsed into semantic chunks`);
    console.log(`[${new Date().toISOString()}] ï¿½ AST fallback: ${astStats.fallback} files used regular splitting (no clear semantic boundaries)`);
    console.log(`[${new Date().toISOString()}] âŒ AST failed: ${astStats.failed} files had parsing errors`);
    console.log(`[${new Date().toISOString()}] ðŸ“Š Total chunks created: ${allSplitDocs.length} optimized for vector retrieval`);
    console.log(`[${new Date().toISOString()}] ðŸŽ¯ Each chunk preserves logical context and semantic meaning for better AI understanding`);
    
    return allSplitDocs;
  }

  /**
   * Regular text splitting for non-code files
   */
  async regularSplitDocument(document) {
    const splitter = new RecursiveCharacterTextSplitter({
      chunkSize: 1500,
      chunkOverlap: 250
    });
    return await splitter.splitDocuments([document]);
  }

  /**
   * Log semantic analysis summary to provide insights into the processed documents
   */
  logSemanticAnalysisSummary(documents) {
    const semanticStats = {
      roles: {},
      layers: {},
      modules: {},
      entryPoints: 0,
      complexity: { high: 0, medium: 0, low: 0 },
      enhanced: 0
    };

    documents.forEach(doc => {
      if (doc.metadata.enhanced) {
        semanticStats.enhanced++;
        
        // Count roles
        const role = doc.metadata.semantic_role || 'unknown';
        semanticStats.roles[role] = (semanticStats.roles[role] || 0) + 1;
        
        // Count layers
        const layer = doc.metadata.layer || 'unknown';
        semanticStats.layers[layer] = (semanticStats.layers[layer] || 0) + 1;
        
        // Count modules
        const module = doc.metadata.eventstorm_module || 'unknown';
        semanticStats.modules[module] = (semanticStats.modules[module] || 0) + 1;
        
        // Count entry points
        if (doc.metadata.is_entrypoint) {
          semanticStats.entryPoints++;
        }
        
        // Count complexity
        const complexity = doc.metadata.complexity || 'unknown';
        if (complexity in semanticStats.complexity) {
          semanticStats.complexity[complexity]++;
        }
      }
    });

    console.log(`[${new Date().toISOString()}] ðŸ§  SEMANTIC ANALYSIS SUMMARY:`);
    console.log(`[${new Date().toISOString()}] ðŸ“Š Enhanced documents: ${semanticStats.enhanced}/${documents.length}`);
    console.log(`[${new Date().toISOString()}] ðŸ“Š Entry points detected: ${semanticStats.entryPoints}`);
    
    console.log(`[${new Date().toISOString()}] ðŸ“Š Semantic roles:`);
    Object.entries(semanticStats.roles).forEach(([role, count]) => {
      console.log(`[${new Date().toISOString()}]    ${role}: ${count}`);
    });
    
    console.log(`[${new Date().toISOString()}] ðŸ“Š Architectural layers:`);
    Object.entries(semanticStats.layers).forEach(([layer, count]) => {
      console.log(`[${new Date().toISOString()}]    ${layer}: ${count}`);
    });
    
    console.log(`[${new Date().toISOString()}] ðŸ“Š EventStorm modules:`);
    Object.entries(semanticStats.modules).forEach(([module, count]) => {
      console.log(`[${new Date().toISOString()}]    ${module}: ${count}`);
    });
    
    console.log(`[${new Date().toISOString()}] ðŸ“Š Complexity distribution:`);
    Object.entries(semanticStats.complexity).forEach(([level, count]) => {
      console.log(`[${new Date().toISOString()}]    ${level}: ${count}`);
    });
    
    console.log(`[${new Date().toISOString()}] ${'â•'.repeat(80)}`);
  }

  /**
   * Log processing pipeline summary to show ubiquitous language and semantic enhancements
   */
  logProcessingPipelineSummary(ubiqDocs, semanticDocs) {
    // Analyze ubiquitous language enhancements
    const ubiqStats = {
      businessModules: {},
      boundedContexts: {},
      totalTermsFound: 0,
      enhanced: 0
    };

    ubiqDocs.forEach(doc => {
      if (doc.metadata.ubiq_business_module) {
        const module = doc.metadata.ubiq_business_module;
        ubiqStats.businessModules[module] = (ubiqStats.businessModules[module] || 0) + 1;
        ubiqStats.enhanced++;
      }
      
      if (doc.metadata.ubiq_bounded_context) {
        const context = doc.metadata.ubiq_bounded_context;
        ubiqStats.boundedContexts[context] = (ubiqStats.boundedContexts[context] || 0) + 1;
      }

      if (doc.metadata.ubiq_terminology && Array.isArray(doc.metadata.ubiq_terminology)) {
        ubiqStats.totalTermsFound += doc.metadata.ubiq_terminology.length;
      }
    });

    // Analyze semantic preprocessing results
    const semanticStats = {
      roles: {},
      layers: {},
      entryPoints: 0,
      enhanced: 0
    };

    semanticDocs.forEach(doc => {
      if (doc.metadata.enhanced) {
        semanticStats.enhanced++;
        
        const role = doc.metadata.semantic_role || 'unknown';
        semanticStats.roles[role] = (semanticStats.roles[role] || 0) + 1;
        
        const layer = doc.metadata.layer || 'unknown';
        semanticStats.layers[layer] = (semanticStats.layers[layer] || 0) + 1;
        
        if (doc.metadata.is_entrypoint) {
          semanticStats.entryPoints++;
        }
      }
    });

    console.log(`[${new Date().toISOString()}] ðŸ“Š PROCESSING PIPELINE SUMMARY:`);
    console.log(`[${new Date().toISOString()}] ${'â•'.repeat(80)}`);
    
    console.log(`[${new Date().toISOString()}] ðŸ“š UBIQUITOUS LANGUAGE ENHANCEMENT:`);
    console.log(`[${new Date().toISOString()}]    Enhanced documents: ${ubiqStats.enhanced}/${ubiqDocs.length}`);
    console.log(`[${new Date().toISOString()}]    Domain terms found: ${ubiqStats.totalTermsFound}`);
    
    console.log(`[${new Date().toISOString()}]    Business modules detected:`);
    Object.entries(ubiqStats.businessModules).forEach(([module, count]) => {
      console.log(`[${new Date().toISOString()}]      ${module}: ${count} documents`);
    });
    
    console.log(`[${new Date().toISOString()}]    Bounded contexts:`);
    Object.entries(ubiqStats.boundedContexts).forEach(([context, count]) => {
      console.log(`[${new Date().toISOString()}]      ${context}: ${count} documents`);
    });

    console.log(`[${new Date().toISOString()}] ðŸ§  SEMANTIC PREPROCESSING:`);
    console.log(`[${new Date().toISOString()}]    Enhanced documents: ${semanticStats.enhanced}/${semanticDocs.length}`);
    console.log(`[${new Date().toISOString()}]    Entry points: ${semanticStats.entryPoints}`);
    
    console.log(`[${new Date().toISOString()}]    Semantic roles:`);
    Object.entries(semanticStats.roles).forEach(([role, count]) => {
      console.log(`[${new Date().toISOString()}]      ${role}: ${count}`);
    });
    
    console.log(`[${new Date().toISOString()}]    Architectural layers:`);
    Object.entries(semanticStats.layers).forEach(([layer, count]) => {
      console.log(`[${new Date().toISOString()}]      ${layer}: ${count}`);
    });
    
    console.log(`[${new Date().toISOString()}] ${'â•'.repeat(80)}`);
  }

  /**
   * Store documents to Pinecone vector database
   */
  async storeToPinecone(documents, namespace, githubOwner, repoName) {
    console.log(`[${new Date().toISOString()}] ðŸ—„ï¸ PINECONE STORAGE EXPLANATION: Converting ${documents?.length || 0} document chunks into searchable vector embeddings`);
    console.log(`[${new Date().toISOString()}] ðŸŽ¯ Each document chunk will be processed by OpenAI's text-embedding-3-large model to create high-dimensional vectors that capture semantic meaning. These vectors are then stored in Pinecone with unique IDs and metadata for lightning-fast similarity search during RAG queries`);
    
    if (!this.pinecone) {
      console.warn(`[${new Date().toISOString()}] âš ï¸ DATA-PREP: Pinecone client not initialized, cannot store documents`);
      console.warn(`[${new Date().toISOString()}] ðŸ’¡ Vector storage skipped - ensure Pinecone credentials are properly configured`);
      return;
    }

    if (!documents || documents.length === 0) {
      console.log(`[${new Date().toISOString()}] âš ï¸ DATA-PREP: No documents to store in Pinecone`);
      console.log(`[${new Date().toISOString()}] ðŸŽ¯ This usually means no processable files were found in the repository`);
      return;
    }

    console.log(`[${new Date().toISOString()}] ðŸŽ¯ STORAGE STRATEGY: Using namespace '${namespace}' for data isolation and generating unique IDs for ${documents.length} chunks from ${githubOwner}/${repoName}`);

    try {
      const index = this.pinecone.Index(process.env.PINECONE_INDEX_NAME || 'eventstorm-index');
      const vectorStore = new PineconeStore(this.embeddings, {
        pineconeIndex: index,
        namespace: namespace // Could be userId for user docs or 'core-docs' for system docs
      });

      console.log(`[${new Date().toISOString()}] ðŸ”‘ ID GENERATION: Creating unique identifiers to prevent collisions and enable precise retrieval`);

      // Generate unique document IDs
      const documentIds = documents.map((doc, index) => {
        const source = this.sanitizeId(doc.metadata.source || 'unknown');
        const repoId = this.sanitizeId(doc.metadata.repoId || 'unknown');
        return `${githubOwner}_${repoId}_${source}_chunk_${index}`;
      });

      console.log(`[${new Date().toISOString()}] âš¡ RATE LIMITING: Using bottleneck limiter to respect Pinecone API limits and prevent throttling`);

      // Use bottleneck limiter for Pinecone operations if available
      if (this.pineconeLimiter) {
        await this.pineconeLimiter.schedule(async () => {
          await vectorStore.addDocuments(documents, { ids: documentIds });
        });
      } else {
        console.log(`[${new Date().toISOString()}] ðŸš€ EMBEDDING & UPLOAD: Processing ${documents.length} chunks directly (no rate limiter)`);
        await vectorStore.addDocuments(documents, { ids: documentIds });
      }

      console.log(`[${new Date().toISOString()}] âœ… DATA-PREP: Successfully stored ${documents.length} chunks to Pinecone namespace: ${namespace}`);
      console.log(`[${new Date().toISOString()}] ðŸŽ¯ STORAGE COMPLETE: Vector embeddings are now searchable via semantic similarity queries in the RAG pipeline`);
      console.log(`[${new Date().toISOString()}] ðŸ“Š Each chunk includes rich metadata (file types, business modules, architectural layers, AST semantic units) for precise context retrieval`);

    } catch (error) {
      console.error(`[${new Date().toISOString()}] âŒ DATA-PREP: Error storing documents to Pinecone:`, error);
      console.error(`[${new Date().toISOString()}] ðŸ’¡ This may be due to API limits, network issues, or invalid document format`);
      throw error;
    }
  }

  /**
   * Sanitize string for use as Pinecone document ID
   */
  sanitizeId(str) {
    return str.replace(/[^a-zA-Z0-9_-]/g, '_').toLowerCase();
  }

  /**
   * Process core documents (API specs, markdown docs) through the full pipeline
   * This ensures consistent processing with ubiquitous language enhancement
   */
  async processDocuments(documents, repoId, githubOwner, repoName) {
    console.log(`[${new Date().toISOString()}] ðŸ“š CORE DOCS PROCESSING EXPLANATION: Running ${documents.length} system documents through complete RAG enhancement pipeline`);
    console.log(`[${new Date().toISOString()}] ðŸŽ¯ This ensures core documentation (API specs, architecture docs, business modules) receives the same domain knowledge enhancement and semantic processing as repository code, maintaining consistency across all stored knowledge`);

    console.log(`[${new Date().toISOString()}] ðŸ“š DATA-PREP: Processing ${documents.length} core documents through full pipeline...`);

    if (!documents || documents.length === 0) {
      console.log(`[${new Date().toISOString()}] âš ï¸ DATA-PREP: No documents provided for processing`);
      return [];
    }

    try {
      console.log(`[${new Date().toISOString()}] ðŸ·ï¸  METADATA ENHANCEMENT: Adding system-level metadata for consistent tracking and retrieval`);
      
      // Add metadata for consistency with repository processing
      const enhancedDocuments = documents.map(doc => ({
        pageContent: doc.pageContent,
        metadata: {
          ...doc.metadata,
          userId: 'system', // Core docs are system-wide
          repoId,
          repoUrl: `https://github.com/${githubOwner}/${repoName}`,
          githubOwner,
          repoName: repoName,
          processedAt: new Date().toISOString(),
          processedBy: 'DataPreparationPipeline-CoreDocs',
          fileType: this.getFileType(doc.metadata.source || '')
        }
      }));

      // Step 1: Enhance with ubiquitous language context (first step in RAG pipeline)
      console.log(`[${new Date().toISOString()}] ðŸ“š DATA-PREP: Applying ubiquitous language enhancement to core docs...`);
      const ubiqLanguageEnhancedDocs = [];
      for (const doc of enhancedDocuments) {
        try {
          const ubiqEnhancedDoc = this.enhanceWithUbiquitousLanguage(doc);
          ubiqLanguageEnhancedDocs.push(ubiqEnhancedDoc);
        } catch (error) {
          console.warn(`[${new Date().toISOString()}] âš ï¸ DATA-PREP: Failed to enhance ${doc.metadata.source} with ubiquitous language: ${error.message}`);
          // Fall back to original document if enhancement fails
          ubiqLanguageEnhancedDocs.push(doc);
        }
      }
      
      console.log(`[${new Date().toISOString()}] ðŸ“š DATA-PREP: Ubiquitous language enhancement completed for ${ubiqLanguageEnhancedDocs.length} core documents`);

      // Step 2: Apply semantic preprocessing to enhance chunks with technical context
      console.log(`[${new Date().toISOString()}] ðŸ§  DATA-PREP: Applying semantic preprocessing to core docs...`);
      const semanticallyEnhancedDocs = [];
      for (const doc of ubiqLanguageEnhancedDocs) {
        try {
          const enhancedDoc = await this.semanticPreprocessor.preprocessChunk(doc);
          semanticallyEnhancedDocs.push(enhancedDoc);
        } catch (error) {
          console.warn(`[${new Date().toISOString()}] âš ï¸ DATA-PREP: Failed to semantically enhance ${doc.metadata.source}: ${error.message}`);
          // Fall back to original document if enhancement fails
          semanticallyEnhancedDocs.push(doc);
        }
      }

      console.log(`[${new Date().toISOString()}] ðŸ§  DATA-PREP: Semantic preprocessing completed for ${semanticallyEnhancedDocs.length} core documents`);

      // Step 3: Apply intelligent splitting (markdown-aware for .md files, AST for code, regular for others)
      console.log(`[${new Date().toISOString()}] âœ‚ï¸ DATA-PREP: Applying intelligent splitting to core documents...`);
      const finalChunks = await this.intelligentSplitDocuments(semanticallyEnhancedDocs, githubOwner, repoName);
      
      console.log(`[${new Date().toISOString()}] âœ‚ï¸ DATA-PREP: Intelligent splitting completed: ${finalChunks.length} chunks created from ${semanticallyEnhancedDocs.length} documents`);

      // Step 4: Store in Pinecone vector database
      await this.storeToPinecone(finalChunks, 'core-docs', githubOwner, repoName);

      // Log processing pipeline summary
      this.logProcessingPipelineSummary('Core Documents', documents.length, finalChunks.length, {
        ubiquitousLanguage: true,
        semanticPreprocessing: true,
        astBasedSplitting: true,
        namespace: 'core-docs'
      });

      return finalChunks;

    } catch (error) {
      console.error(`[${new Date().toISOString()}] âŒ DATA-PREP: Error processing core documents:`, error);
      throw error;
    }
  }
}

module.exports = DataPreparationPipeline;
