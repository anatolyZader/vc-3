// aiLangchainAdapter.js
"use strict";
/* eslint-disable no-unused-vars */

/**
 * =========================================================================
 * AI LANGCHAIN ADAPTER - COMPREHENSIVE DOCUMENTATION & INDEX
 * =========================================================================
 * 
 * This file implements a Retrieval-Augmented Generation (RAG) system that:
 * 1. Indexes repository code and documentation into vector databases
 * 2. Provides AI-powered responses using retrieved context
 * 3. Manages rate limiting and queue processing
 * 4. Handles multiple LLM providers (OpenAI, Anthropic, Google)
 * 
 * ARCHITECTURE OVERVIEW:
 * =====================
 * 
 * DATA PREPARATION PHASE (Heavy Operations - Done Once Per Repo):
 * - processPushedRepo(): Main entry point for repository processing
 * - indexCoreDocsToPinecone(): Indexes API specs and markdown docs
 * - Uses GithubRepoLoader to fetch and chunk repository files
 * - Stores embeddings in Pinecone vector database with namespaces
 * 
 * QUERY PHASE (Lightweight - Per User Request):
 * - respondToPrompt(): Main AI response generation with RAG
 * - generateStandardResponse(): Fallback without vector retrieval
 * - Accepts conversation history from service layer (no database access)
 * 
 * MAIN CLASS & METHODS INDEX:
 * ==========================
 * 
 * CLASS: AILangchainAdapter extends IAIPort
 * â”œâ”€â”€ CONSTRUCTOR & SETUP
 * â”‚   â”œâ”€â”€ constructor(options) [Line 372] - Initialize adapter with rate limiting, LLM setup
 * â”‚   â”œâ”€â”€ setUserId(userId) [Line 461] - Set current user context
 * â”‚   â””â”€â”€ initializeLLM() [Line 494] - Configure LLM provider (OpenAI/Anthropic/Google)
 * â”‚
 * â”œâ”€â”€ CORE INDEXING METHODS (Data Preparation)
 * â”‚   â”œâ”€â”€ processPushedRepo(userId, repoId, repoData) [Line 616] 
 * â”‚   â”‚   â””â”€â”€ Main entry point for repository indexing pipeline
 * â”‚   â”‚   â””â”€â”€ Loads repo files, creates embeddings, stores in Pinecone
 * â”‚   â”‚   â””â”€â”€ Emits progress events via RAG status
 * â”‚   â”‚
 * â”‚   â”œâ”€â”€ indexCoreDocsToPinecone() [Line 155]
 * â”‚   â”‚   â””â”€â”€ Indexes API specifications and markdown documentation
 * â”‚   â”‚   â””â”€â”€ Chunks API spec into tags, endpoints, info sections
 * â”‚   â”‚   â””â”€â”€ Processes markdown files from specific directories
 * â”‚   â”‚
 * â”‚   â”œâ”€â”€ loadApiSpec(filePath) [Line 260]
 * â”‚   â”‚   â””â”€â”€ Loads and validates httpApiSpec.json file
 * â”‚   â”‚   â””â”€â”€ Returns structured document for embedding
 * â”‚   â”‚
 * â”‚   â”œâ”€â”€ loadMarkdownFiles() [Line 295]
 * â”‚   â”‚   â””â”€â”€ Scans for .md files in root and module directories
 * â”‚   â”‚   â””â”€â”€ Categorizes docs as root/architecture/module documentation
 * â”‚   â”‚
 * â”‚   â”œâ”€â”€ chunkApiSpecEndpoints(apiSpecJson, documents) [Line 388]
 * â”‚   â”‚   â””â”€â”€ Creates endpoint-specific chunks from OpenAPI spec
 * â”‚   â”‚   â””â”€â”€ Each endpoint becomes a detailed, searchable document
 * â”‚   â”‚
 * â”‚   â”œâ”€â”€ chunkApiSpecSchemas(apiSpecJson, documents) [Line 448]
 * â”‚   â”‚   â””â”€â”€ Creates component schema chunks from OpenAPI spec
 * â”‚   â”‚   â””â”€â”€ Each schema definition becomes a structured document
 * â”‚   â”‚
 * â”‚   â””â”€â”€ splitMarkdownDocuments(markdownDocs) [Line 487]
 * â”‚       â””â”€â”€ Uses header-aware splitting for markdown files
 * â”‚       â””â”€â”€ Preserves document structure and section context
 * â”‚
 * â”œâ”€â”€ AI RESPONSE GENERATION (Query Phase)
 * â”‚   â”œâ”€â”€ respondToPrompt(userId, conversationId, prompt, conversationHistory) [Line 757]
 * â”‚   â”‚   â””â”€â”€ PRIMARY METHOD: Generates AI responses using RAG
 * â”‚   â”‚   â””â”€â”€ Flow: Rate limit â†’ Vector search â†’ LLM generation
 * â”‚   â”‚   â””â”€â”€ Combines: passed conversation history + retrieved docs + user prompt
 * â”‚   â”‚   â””â”€â”€ Falls back to generateStandardResponse() if vector search fails
 * â”‚   â”‚   â””â”€â”€ NO DATABASE ACCESS - history passed from service layer
 * â”‚   â”‚
 * â”‚   â””â”€â”€ generateStandardResponse(prompt, conversationId, conversationHistory) [Line 1148]
 * â”‚       â””â”€â”€ FALLBACK METHOD: AI response without vector retrieval
 * â”‚       â””â”€â”€ Uses only passed conversation history + API spec summary
 * â”‚       â””â”€â”€ Simpler but less context-aware than full RAG
 * â”‚       â””â”€â”€ NO DATABASE ACCESS - history passed as parameter
 * â”‚
 * â”œâ”€â”€ CONVERSATION MANAGEMENT
 * â”‚   â””â”€â”€ formatConversationHistory(history) [Line 703]
 * â”‚       â””â”€â”€ Converts database records to LangChain message format
 * â”‚       â””â”€â”€ Handles user/assistant message types
 * â”‚       â””â”€â”€ UTILITY ONLY - processes data passed from service layer
 * â”‚
 * â”œâ”€â”€ RATE LIMITING & QUEUE MANAGEMENT
 * â”‚   â”œâ”€â”€ checkRateLimit() [Line 574]
 * â”‚   â”‚   â””â”€â”€ Enforces per-user request rate limits using Bottleneck
 * â”‚   â”‚   â””â”€â”€ Prevents API abuse and cost overrun
 * â”‚   â”‚
 * â”‚   â”œâ”€â”€ waitWithBackoff(retryCount) [Line 598]
 * â”‚   â”‚   â””â”€â”€ Implements exponential backoff for retries
 * â”‚   â”‚   â””â”€â”€ Used when rate limits or API errors occur
 * â”‚   â”‚
 * â”‚   â”œâ”€â”€ processQueue() [Line 1378]
 * â”‚   â”‚   â””â”€â”€ Background queue processor for managing concurrent requests
 * â”‚   â”‚   â””â”€â”€ Ensures orderly processing and prevents overwhelming APIs
 * â”‚   â”‚
 * â”‚   â””â”€â”€ queueRequest(execute) [Line 1417]
 * â”‚       â””â”€â”€ Adds requests to processing queue
 * â”‚       â””â”€â”€ Returns promise that resolves when request is processed
 * â”‚
 * â”œâ”€â”€ UTILITY METHODS
 * â”‚   â”œâ”€â”€ formatApiSpecSummary(apiSpec) [Line 733]
 * â”‚   â”‚   â””â”€â”€ Extracts key info from API specification
 * â”‚   â”‚   â””â”€â”€ Used when full vector search isn't available
 * â”‚   â”‚
 * â”‚   â”œâ”€â”€ getFileType(filePath) [Line 1236]
 * â”‚   â”‚   â””â”€â”€ Determines file category for better indexing
 * â”‚   â”‚   â””â”€â”€ Returns: code, documentation, configuration, or other
 * â”‚   â”‚
 * â”‚   â”œâ”€â”€ sanitizeId(input) [Line 1272]
 * â”‚   â”‚   â””â”€â”€ Cleans strings for use as vector store namespaces
 * â”‚   â”‚   â””â”€â”€ Ensures valid Pinecone namespace format
 * â”‚   â”‚
 * â”‚   â”œâ”€â”€ emitRagStatus(status, details) [Line 1278]
 * â”‚   â”‚   â””â”€â”€ Emits progress events during indexing operations
 * â”‚   â”‚   â””â”€â”€ Used for real-time status updates to clients
 * â”‚   â”‚
 * â”‚   â””â”€â”€ createSmartSplitter(documents) [Line 1611]
 * â”‚   â”‚   â””â”€â”€ Creates language-specific and content-aware text splitters
 * â”‚   â”‚   â””â”€â”€ Optimizes chunking based on code language and file type detection
 * â”‚   â”‚
 * â”‚   â””â”€â”€ getCodeAwareSeparators(language) [Line 1671]
 * â”‚       â””â”€â”€ Returns language-specific separators for better code chunking
 * â”‚       â””â”€â”€ Ensures chunks break at logical code boundaries
 * â”‚
 * EXTERNAL DEPENDENCIES:
 * ======================
 * - @langchain/openai: OpenAI LLM and embeddings
 * - @langchain/anthropic: Anthropic Claude models
 * - @langchain/google-genai: Google Gemini models
 * - @langchain/pinecone: Vector store integration
 * - @langchain/community: GitHub loader and other tools
 * - @langchain/textsplitters: Smart text chunking
 * - bottleneck: Rate limiting implementation
 * - @pinecone-database/pinecone: Vector database client
 * 
 * KEY CONFIGURATION:
 * ==================
 * - Vector embeddings: OpenAI text-embedding-3-large
 * - Vector store: Pinecone with separate namespaces per repository
 * - Text chunking: 1500 char chunks with 250 char overlap
 * - Rate limits: 60 requests per minute default
 * - Conversation history: Passed from service (no direct DB access)
 * - Supported file types: .js, .ts, .md, .json, .yml, .yaml, .py, etc.
 * 
 * ERROR HANDLING:
 * ===============
 * - Graceful fallbacks when vector search fails
 * - Rate limit handling with exponential backoff
 * - Comprehensive logging with timestamps
 * - Progress tracking via RAG status emissions
 * - Timeout handling for vector operations (30s default)
 * 
 * PERFORMANCE CONSIDERATIONS:
 * ===========================
 * - Lazy initialization of LLM clients
 * - Efficient vector similarity search with metadata filtering
 * - Smart chunking to balance context vs. relevance
 * - Background queue processing for concurrent requests
 * - Namespace isolation to prevent cross-repository contamination
 * - Language-specific text splitters for optimal code chunking
 * 
 * ARCHITECTURAL IMPROVEMENTS (Post-Persistence Isolation):
 * =======================================================
 * âœ… REMOVED: Direct database access and persistence adapter dependency
 * âœ… IMPROVED: Clean separation between AI operations and data persistence
 * âœ… ENHANCED: Service layer now handles all conversation history management
 * âœ… SIMPLIFIED: Adapter focused purely on AI/RAG functionality
 * âœ… MAINTAINED: Full functionality with better architectural boundaries
 */

const IAIPort = require('../../domain/ports/IAIPort');
const Bottleneck = require("bottleneck");
const { GithubRepoLoader } = require("@langchain/community/document_loaders/web/github");
const { RecursiveCharacterTextSplitter } = require("@langchain/textsplitters");
const { MarkdownHeaderTextSplitter } = require("@langchain/textsplitters");
const { PineconeStore } = require('@langchain/pinecone');
const { Pinecone } = require('@pinecone-database/pinecone');
const { OpenAIEmbeddings } = require('@langchain/openai');

// Import the new prompt systemm
const { SystemPrompts, PromptSelector } = require('./prompts/systemPrompts');
const PromptConfig = require('./prompts/promptConfig');

class AILangchainAdapter extends IAIPort {
  // Separate method to index only core docs (can be called independently)
  // Remove this unused method

  // Automate indexing of httpApiSpec.json and markdown documentation into Pinecone
  async indexCoreDocsToPinecone() {
    console.log(`[${new Date().toISOString()}] ðŸ”µ [RAG-INDEX] Starting core docs indexing into 'core-docs' namespace.`);
    // Load API spec
    const apiSpecChunk = await this.loadApiSpec('httpApiSpec.json');
    // Load markdown documentation files
    const markdownDocs = await this.loadMarkdownFiles();

    const documents = [];
    if (apiSpecChunk) {
      console.log(`[${new Date().toISOString()}] ðŸ”µ [RAG-INDEX] API Spec loaded successfully.`);
      // Parse JSON for targeted chunking
      let apiSpecJson;
      try {
        apiSpecJson = JSON.parse(apiSpecChunk.pageContent);
      } catch (e) {
        apiSpecJson = null;
      }
      
      if (apiSpecJson) {
        // Create endpoint-specific chunks for better retrieval
        await this.chunkApiSpecEndpoints(apiSpecJson, documents);
        
        // Create schema chunks from components
        await this.chunkApiSpecSchemas(apiSpecJson, documents);
        
        // Tags chunk (keep existing)
        if (Array.isArray(apiSpecJson.tags)) {
          const tagsText = apiSpecJson.tags.map(tag => `- ${tag.name}: ${tag.description}`).join('\n');
          documents.push({
            pageContent: `API Tags:\n${tagsText}`,
            metadata: { source: 'httpApiSpec.json', type: 'apiSpecTags' }
          });
        }
        
        // Info chunkk (keep existing)
        if (apiSpecJson.info) {
          documents.push({
            pageContent: `API Info:\nTitle: ${apiSpecJson.info.title}\nDescription: ${apiSpecJson.info.description}\nVersion: ${apiSpecJson.info.version}`,
            metadata: { source: 'httpApiSpec.json', type: 'apiSpecInfo' }
          });
        }
      }
    }

    // Add markdown documentation files
    if (markdownDocs.length > 0) {
      console.log(`[${new Date().toISOString()}] ðŸ”µ [RAG-INDEX] ${markdownDocs.length} Markdown docs loaded.`);
      documents.push(...markdownDocs);
    }

    if (documents.length === 0) {
      console.log(`[${new Date().toISOString()}] ðŸŸ¡ [RAG-INDEX] No core documents found to index. Aborting.`);
      return;
    }

    // Use improved splitter for chunking markdown docs
    const docsToSplit = documents.filter(doc => 
      doc.metadata.type === 'root_documentation' || 
      doc.metadata.type === 'module_documentation' ||
      doc.metadata.type === 'architecture_documentation'
    );
    let splittedDocs = [];
    if (docsToSplit.length > 0) {
      // Use markdown-aware splitter for semantic chunking
      splittedDocs = await this.splitMarkdownDocuments(docsToSplit);
    }
    
    // Add all API spec chunks (already optimally chunked)
    splittedDocs = splittedDocs.concat(documents.filter(doc => doc.metadata.source === 'httpApiSpec.json'));
    console.log(`[${new Date().toISOString()}] ðŸ”µ [RAG-INDEX] Total documents after splitting: ${splittedDocs.length}`);

    // Generate unique IDs for Pinecone
    const repoId = 'core-docs';
    const documentIds = splittedDocs.map((doc, index) =>
      `system_${repoId}_${this.sanitizeId(doc.metadata.type || doc.metadata.source || 'unknown')}_chunk_${index}`
    );

    // Store in Pinecone in a GLOBAL namespacee
    if (this.pinecone) {
        try {
            const coreDocsIndex = this.pinecone.Index(process.env.PINECONE_INDEX_NAME || 'eventstorm-index');
            const coreDocsVectorStore = new PineconeStore(this.embeddings, {
                pineconeIndex: coreDocsIndex,
                namespace: 'core-docs' // Using a fixed, global namespace
            });
            await coreDocsVectorStore.addDocuments(splittedDocs, { ids: documentIds });
            console.log(`[${new Date().toISOString()}] âœ… [RAG-INDEX] Successfully indexed ${splittedDocs.length} core doc chunks to Pinecone in 'core-docs' namespace.`);
        } catch (error) {
            console.error(`[${new Date().toISOString()}] âŒ [RAG-INDEX] Error indexing core docs to Pinecone:`, error);
        }
    } else {
      console.warn(`[${new Date().toISOString()}] âš ï¸ [RAG-INDEX] Pinecone client not initialized, cannot index core docs.`);
    }
  }
  // Helper to load JSON spec file from backend root
  async loadApiSpec(filePath) {
    const fs = require('fs');
    const path = require('path');
    const backendRoot = path.resolve(__dirname, '../../../..');
    const absPath = path.resolve(backendRoot, filePath);
    try {
      const content = await fs.promises.readFile(absPath, 'utf8');
      // Optionally parse and pretty-print JSON
      let prettyContent = content;
      try {
        const json = JSON.parse(content);
        prettyContent = JSON.stringify(json, null, 2);
      } catch (e) {}
      return {
        pageContent: prettyContent,
        metadata: { source: 'httpApiSpec.json', type: 'apiSpec' }
      };
    } catch (err) {
      console.warn(`[${new Date().toISOString()}] Could not load API spec file at ${absPath}: ${err.message}`);
      return null;
    }
  }

  // Helper to load markdown files from root directory and business modules
  async loadMarkdownFiles() {
    const fs = require('fs');
    const path = require('path');
    const markdownDocs = [];
    
    try {
      const backendRoot = path.resolve(__dirname, '../../../..');
      
      // Load ROOT_DOCUMENTATION.md from backend root if it exists
      const rootDocPath = path.join(backendRoot, 'ROOT_DOCUMENTATION.md');
      try {
        const rootDocContent = await fs.promises.readFile(rootDocPath, 'utf8');
        markdownDocs.push({
          pageContent: rootDocContent,
          metadata: { 
            source: 'ROOT_DOCUMENTATION.md', 
            type: 'root_documentation',
            priority: 'high'
          }
        });
        console.log(`[${new Date().toISOString()}] [DEBUG] Loaded root documentation file`);
      } catch (err) {
        console.log(`[${new Date().toISOString()}] [DEBUG] No root documentation file found at ${rootDocPath}`);
      }

      // Load ARCHITECTURE.md from backend root if it exists
      const archDocPath = path.join(backendRoot, 'ARCHITECTURE.md');
      try {
        const archDocContent = await fs.promises.readFile(archDocPath, 'utf8');
        markdownDocs.push({
          pageContent: archDocContent,
          metadata: { 
            source: 'ARCHITECTURE.md', 
            type: 'architecture_documentation',
            priority: 'high'
          }
        });
        console.log(`[${new Date().toISOString()}] [DEBUG] Loaded architecture documentation file`);
      } catch (err) {
        console.log(`[${new Date().toISOString()}] [DEBUG] No architecture documentation file found at ${archDocPath}`);
      }

      // Load markdown files from business modules
      const businessModulesPath = path.join(backendRoot, 'business_modules');
      
      try {
        const modules = await fs.promises.readdir(businessModulesPath, { withFileTypes: true });
        
        for (const module of modules) {
          if (module.isDirectory()) {
            const modulePath = path.join(businessModulesPath, module.name);
            const moduleMarkdownPath = path.join(modulePath, `${module.name}.md`);
            
            try {
              const moduleContent = await fs.promises.readFile(moduleMarkdownPath, 'utf8');
              markdownDocs.push({
                pageContent: moduleContent,
                metadata: { 
                  source: `business_modules/${module.name}/${module.name}.md`, 
                  type: 'module_documentation',
                  module: module.name,
                  priority: 'medium'
                }
              });
              console.log(`[${new Date().toISOString()}] [DEBUG] Loaded ${module.name} module documentation`);
            } catch (err) {
              console.log(`[${new Date().toISOString()}] [DEBUG] No documentation file found for module ${module.name} at ${moduleMarkdownPath}`);
            }
          }
        }
      } catch (err) {
        console.warn(`[${new Date().toISOString()}] Could not read business modules directory: ${err.message}`);
      }

      // Sort by priority (high priority first)
      markdownDocs.sort((a, b) => {
        const priorityOrder = { 'high': 0, 'medium': 1, 'low': 2 };
        return priorityOrder[a.metadata.priority] - priorityOrder[b.metadata.priority];
      });

      console.log(`[${new Date().toISOString()}] [DEBUG] Loaded ${markdownDocs.length} total markdown documentation files`);
      return markdownDocs;
      
    } catch (err) {
      console.error(`[${new Date().toISOString()}] Error loading markdown files: ${err.message}`);
      return [];
    }
  }

  // Enhanced API spec chunking - creates endpoint-specific chunks
  async chunkApiSpecEndpoints(apiSpecJson, documents) {
    if (!apiSpecJson.paths || typeof apiSpecJson.paths !== 'object') {
      console.log(`[${new Date().toISOString()}] ðŸ”µ [RAG-INDEX] No paths found in API spec`);
      return;
    }

    let endpointCount = 0;
    Object.entries(apiSpecJson.paths).forEach(([path, methods]) => {
      Object.entries(methods).forEach(([method, operation]) => {
        if (typeof operation !== 'object') return; // Skip non-operation entries

        const endpoint = {
          method: method.toUpperCase(),
          path: path,
          summary: operation.summary || '',
          description: operation.description || '',
          tags: operation.tags || [],
          operationId: operation.operationId || ''
        };

        // Build comprehensive endpoint documentation
        let content = `${endpoint.method} ${endpoint.path}\n`;
        if (endpoint.summary) content += `Summary: ${endpoint.summary}\n`;
        if (endpoint.description) content += `Description: ${endpoint.description}\n`;
        if (endpoint.operationId) content += `Operation ID: ${endpoint.operationId}\n`;
        if (endpoint.tags.length > 0) content += `Tags: ${endpoint.tags.join(', ')}\n`;

        // Add parameters information
        if (operation.parameters && Array.isArray(operation.parameters)) {
          content += `\nParameters:\n`;
          operation.parameters.forEach(param => {
            content += `- ${param.name} (${param.in}): ${param.description || 'No description'} ${param.required ? '[Required]' : '[Optional]'}\n`;
          });
        }

        // Add request body information
        if (operation.requestBody) {
          content += `\nRequest Body:\n`;
          if (operation.requestBody.description) {
            content += `Description: ${operation.requestBody.description}\n`;
          }
          if (operation.requestBody.content) {
            Object.keys(operation.requestBody.content).forEach(mediaType => {
              content += `Media Type: ${mediaType}\n`;
            });
          }
        }

        // Add response information
        if (operation.responses && typeof operation.responses === 'object') {
          content += `\nResponses:\n`;
          Object.entries(operation.responses).forEach(([statusCode, response]) => {
            if (response.description) {
              content += `- ${statusCode}: ${response.description}\n`;
            }
          });
        }

        documents.push({
          pageContent: content,
          metadata: {
            source: 'httpApiSpec.json',
            type: 'api_endpoint',
            method: method.toLowerCase(),
            path: path,
            tags: endpoint.tags,
            operationId: endpoint.operationId || `${method}_${path.replace(/[^a-zA-Z0-9]/g, '_')}`
          }
        });

        endpointCount++;
      });
    });

    console.log(`[${new Date().toISOString()}] ðŸ”µ [RAG-INDEX] Created ${endpointCount} endpoint-specific chunks`);
  }

  // Enhanced API spec schema chunking - creates component schema chunks
  async chunkApiSpecSchemas(apiSpecJson, documents) {
    if (!apiSpecJson.components || !apiSpecJson.components.schemas) {
      console.log(`[${new Date().toISOString()}] ðŸ”µ [RAG-INDEX] No schemas found in API spec components`);
      return;
    }

    let schemaCount = 0;
    Object.entries(apiSpecJson.components.schemas).forEach(([schemaName, schema]) => {
      let content = `Schema: ${schemaName}\n`;
      if (schema.type) content += `Type: ${schema.type}\n`;
      if (schema.description) content += `Description: ${schema.description}\n`;

      // Add properties information
      if (schema.properties && typeof schema.properties === 'object') {
        content += `\nProperties:\n`;
        Object.entries(schema.properties).forEach(([propName, propSchema]) => {
          const required = schema.required && schema.required.includes(propName) ? '[Required]' : '[Optional]';
          const type = propSchema.type || 'unknown';
          const description = propSchema.description || 'No description';
          content += `- ${propName} (${type}): ${description} ${required}\n`;
        });
      }

      // Add enum values if present
      if (schema.enum && Array.isArray(schema.enum)) {
        content += `\nPossible Values: ${schema.enum.join(', ')}\n`;
      }

      documents.push({
        pageContent: content,
        metadata: {
          source: 'httpApiSpec.json',
          type: 'api_schema',
          schemaName: schemaName,
          schemaType: schema.type || 'object'
        }
      });

      schemaCount++;
    });

    console.log(`[${new Date().toISOString()}] ðŸ”µ [RAG-INDEX] Created ${schemaCount} schema-specific chunks`);
  }

  // Enhanced markdown splitting using header-aware splitter
  async splitMarkdownDocuments(markdownDocs) {
    const splittedDocs = [];

    for (const doc of markdownDocs) {
      try {
        // Try header-based splitting first
        const headerSplitter = new MarkdownHeaderTextSplitter({
          headersToSplitOn: [
            { level: 1, name: 'h1' },
            { level: 2, name: 'h2' },
            { level: 3, name: 'h3' }
          ]
        });

        const headerChunks = await headerSplitter.splitDocuments([doc]);
        
        if (headerChunks.length > 1) {
          // Header splitting was successful
          headerChunks.forEach((chunk, index) => {
            const enhancedMetadata = {
              ...doc.metadata,
              chunkIndex: index,
              splitterType: 'header-based'
            };

            // Extract section information from metadata
            if (chunk.metadata) {
              Object.keys(chunk.metadata).forEach(key => {
                if (key.startsWith('h') && chunk.metadata[key]) {
                  enhancedMetadata.section = chunk.metadata[key];
                  enhancedMetadata.sectionLevel = key;
                }
              });
            }

            splittedDocs.push({
              ...chunk,
              metadata: enhancedMetadata
            });
          });

          console.log(`[${new Date().toISOString()}] ðŸ”µ [RAG-INDEX] Split ${doc.metadata.source} into ${headerChunks.length} header-based chunks`);
        } else {
          // Fallback to character-based splitting
          const charSplitter = new RecursiveCharacterTextSplitter({
            chunkSize: 1500,
            chunkOverlap: 250
          });

          const charChunks = await charSplitter.splitDocuments([doc]);
          charChunks.forEach((chunk, index) => {
            splittedDocs.push({
              ...chunk,
              metadata: {
                ...doc.metadata,
                chunkIndex: index,
                splitterType: 'character-based'
              }
            });
          });

          console.log(`[${new Date().toISOString()}] ðŸ”µ [RAG-INDEX] Split ${doc.metadata.source} into ${charChunks.length} character-based chunks (header splitting failed)`);
        }
      } catch (error) {
        console.error(`[${new Date().toISOString()}] Error splitting markdown doc ${doc.metadata.source}:`, error.message);
        // Add the original document if splitting fails
        splittedDocs.push({
          ...doc,
          metadata: {
            ...doc.metadata,
            splitterType: 'none',
            error: 'splitting_failed'
          }
        });
      }
    }

    return splittedDocs;
  }
  constructor(options = {}) {
    super();

    // Make userId null by default to avoid DI error
    this.userId = null;
    console.log(`[${new Date().toISOString()}] [DEBUG] Constructor called with options:`, JSON.stringify(options));

    // Get provider from infraConfig or options
    this.aiProvider = options.aiProvider || 'openai';
    console.log(`[${new Date().toISOString()}] AILangchainAdapter initializing with provider: ${this.aiProvider}`);
    console.log(`[${new Date().toISOString()}] [DEBUG] aiProvider set to: ${this.aiProvider}`);

    // Get access to the event bus for status updates
    try {
      const { eventBus } = require('../../../../eventDispatcher');
      this.eventBus = eventBus;
      console.log(`[${new Date().toISOString()}] ðŸ“¡ Successfully connected to shared event bus`);
      console.log(`[${new Date().toISOString()}] [DEBUG] Event bus connected.`);
    } catch (error) {
      console.warn(`[${new Date().toISOString()}] âš ï¸ Could not access shared event bus: ${error.message}`);
      this.eventBus = null;
      console.log(`[${new Date().toISOString()}] [DEBUG] Event bus unavailable.`);
    }

    // Rate limiting parameters for LLM (keep for LLM calls)
    this.requestsInLastMinute = 0;
    this.lastRequestTime = Date.now();
    this.maxRequestsPerMinute = 60;
    this.retryDelay = 5000;
    this.maxRetries = 10;
    console.log(`[${new Date().toISOString()}] [DEBUG] Rate limiting params: maxRequestsPerMinute=${this.maxRequestsPerMinute}, retryDelay=${this.retryDelay}, maxRetries=${this.maxRetries}`);

    // Bottleneck rate limiter for Pinecone upserts
    this.pineconeLimiter = new Bottleneck({
      reservoir: 100, // 100 records per second
      reservoirRefreshAmount: 100,
      reservoirRefreshInterval: 1000,
      maxConcurrent: 1
    });
    console.log(`[${new Date().toISOString()}] [DEBUG] Bottleneck limiter initialized.`);

    // Request queue system
    this.requestQueue = [];
    this.isProcessingQueue = false;
    console.log(`[${new Date().toISOString()}] [DEBUG] Request queue initialized.`);

    // Start queue processor
    this.startQueueProcessor();
    console.log(`[${new Date().toISOString()}] [DEBUG] Queue processor started.`);

    try {
      // Initialize embeddings model: converts text to vectors
      this.embeddings = new OpenAIEmbeddings({
        model: 'text-embedding-3-large',
        apiKey: process.env.OPENAI_API_KEY
      });
      console.log(`[${new Date().toISOString()}] [DEBUG] Embeddings model initialized.`);

      // Initialize Pinecone client if API key available
      if (process.env.PINECONE_API_KEY) {
        this.pinecone = new Pinecone({
          apiKey: process.env.PINECONE_API_KEY
        });
        console.log(`[${new Date().toISOString()}] [DEBUG] Pinecone client initialized.`);
        console.log(`[${new Date().toISOString()}] [DEBUG] Pinecone environment: ${process.env.PINECONE_ENVIRONMENT}`);
        console.log(`[${new Date().toISOString()}] [DEBUG] Pinecone index name: ${process.env.PINECONE_INDEX_NAME}`);
      } else {
        console.warn(`[${new Date().toISOString()}] No Pinecone API key found, vector search will be unavailable`);
        this.pinecone = null;
        console.log(`[${new Date().toISOString()}] [DEBUG] Pinecone client unavailable.`);
      }

      // Initialize chat model based on provider
      this.initializeLLM();
      console.log(`[${new Date().toISOString()}] [DEBUG] LLM initialized.`);

      // Don't initialize vectorStore until we have a userId
      this.vectorStore = null;
      console.log(`[${new Date().toISOString()}] [DEBUG] Vector store set to null (will be initialized after userId).`);

      console.log(`[${new Date().toISOString()}] AILangchainAdapter initialized successfully`);
    } catch (error) {
      console.error(`[${new Date().toISOString()}] Error initializing AILangchainAdapter:`, error.message);
      console.log(`[${new Date().toISOString()}] [DEBUG] Initialization error stack:`, error.stack);
      // We'll continue with degraded functionality and try to recover later
    }
  }

  // Add method to set userId after construction - this is crucial!
  setUserId(userId) {
    if (!userId) {
      console.warn(`[${new Date().toISOString()}] Attempted to set null/undefined userId in AILangchainAdapter`);
      return this;
    }
    console.log(`[${new Date().toISOString()}] [DEBUG] setUserId called with: ${userId}`);

    this.userId = userId;
    console.log(`[${new Date().toISOString()}] [DEBUG] userId set to: ${this.userId}`);

    // Update vector store namespace with the user ID
    try {
      if (this.pinecone) {
        this.vectorStore = new PineconeStore(this.embeddings, {
          pineconeIndex: this.pinecone.Index(process.env.PINECONE_INDEX_NAME || 'eventstorm-index'),
          namespace: this.userId
        });
        console.log(`[${new Date().toISOString()}] AILangchainAdapter userId updated to: ${this.userId}`);
        console.log(`[${new Date().toISOString()}] [DEBUG] Vector store initialized for userId: ${this.userId}`);
      } else {
        console.warn(`[${new Date().toISOString()}] Pinecone client not available, vectorStore not initialized for user ${userId}`);
        console.log(`[${new Date().toISOString()}] [DEBUG] Vector store NOT initialized (no Pinecone client).`);
      }
    } catch (error) {
      console.error(`[${new Date().toISOString()}] Error creating vector store for user ${this.userId}:`, error.message);
      console.log(`[${new Date().toISOString()}] [DEBUG] Vector store creation error stack:`, error.stack);
      // Still set the userId even if vectorStore creation fails
    }

    return this;
  }

  // Initialize LLM based on provider
  initializeLLM() {
    try {
      switch (this.aiProvider.toLowerCase()) {
        case 'openai': {
          console.log(`[${new Date().toISOString()}] Initializing OpenAI provider`);
          // Import here to avoid requiring if not using this provider
          const { ChatOpenAI } = require('@langchain/openai');
          this.llm = new ChatOpenAI({
            modelName: 'gpt-3.5-turbo',
            temperature: 0,
            apiKey: process.env.OPENAI_API_KEY,
            maxRetries: this.maxRetries,
            maxConcurrency: 2
          });
          break;
        }

        case 'anthropic': {
          console.log(`[${new Date().toISOString()}] Initializing Anthropic provider`);
          // Import here to avoid requiring if not using this provider
          const { ChatAnthropic } = require('@langchain/anthropic');
          this.llm = new ChatAnthropic({
            modelName: 'claude-3-haiku-20240307',
            temperature: 0,
            apiKey: process.env.ANTHROPIC_API_KEY,
            maxRetries: this.maxRetries,
            maxConcurrency: 1, // Reduced to 1 to avoid rate limiting
            streaming: false, // Disable streaming to reduce connection overhead
            timeout: 120000 // 2 minute timeout
          });
          break;
        }

        case 'google': {
          console.log(`[${new Date().toISOString()}] Initializing Google provider`);
          // Import here to avoid requiring if not using this provider
          const { ChatGoogleGenerativeAI } = require('@langchain/google-genai');
          this.llm = new ChatGoogleGenerativeAI({
            modelName: 'gemini-pro',
            apiKey: process.env.GOOGLE_API_KEY,
            maxRetries: this.maxRetries,
            maxConcurrency: 2
          });
          break;
        }

        case 'ollama': {
          console.log(`[${new Date().toISOString()}] Initializing Ollama provider`);
          // Import here to avoid requiring if not using this provider
          const { ChatOllama } = require('@langchain/community/chat_models/ollama');
          this.llm = new ChatOllama({
            model: process.env.OLLAMA_MODEL || 'llama2',
            baseUrl: process.env.OLLAMA_BASE_URL || 'http://localhost:11434',
            maxRetries: this.maxRetries
          });
          break;
        }

        default: {
          console.warn(`[${new Date().toISOString()}] Unknown provider: ${this.aiProvider}, falling back to OpenAI`);
          const { ChatOpenAI: DefaultChatOpenAI } = require('@langchain/openai');
          this.llm = new DefaultChatOpenAI({
            modelName: 'gpt-3.5-turbo',
            temperature: 0,
            apiKey: process.env.OPENAI_API_KEY,
            maxRetries: this.maxRetries,
            maxConcurrency: 2
          });
        }
      }

      console.log(`[${new Date().toISOString()}] Successfully initialized LLM for provider: ${this.aiProvider}`);

    } catch (error) {
      console.error(`[${new Date().toISOString()}] Error initializing LLM for provider ${this.aiProvider}:`, error.message);
      throw new Error(`Failed to initialize LLM provider ${this.aiProvider}: ${error.message}`);
    }
  }

  // Rate limit handling method
  async checkRateLimit() {
    const now = Date.now();
    const timeWindow = 60 * 1000; // 1 minute in milliseconds

    // Reset counter if more than a minute has passed
    if (now - this.lastRequestTime > timeWindow) {
      this.requestsInLastMinute = 0;
      this.lastRequestTime = now;
      return true;
    }

    // If we're still under the limit, increment and allow
    if (this.requestsInLastMinute < this.maxRequestsPerMinute) {
      this.requestsInLastMinute++;
      this.lastRequestTime = now;
      return true;
    }

    // Otherwise, we need to wait
    console.log(`[${new Date().toISOString()}] Rate limit reached, delaying request`);
    return false;
  }

  // Function to wait with improved exponential backoff with jitter
  async waitWithBackoff(retryCount) {
    // Base delay plus linear component to ensure minimum wait time increases with retries
    const baseDelay = this.retryDelay + (retryCount * 5000);

    // Add jitter (Â±10%) to prevent thundering herd problem
    const jitterFactor = 0.9 + (Math.random() * 0.2);

    // Calculate final delay with max cap
    const delay = Math.min(
      baseDelay * jitterFactor,
      60000 // Max 60 seconds (increased from 30s)
    );

    console.log(`[${new Date().toISOString()}] Waiting ${Math.round(delay)}ms before retry ${retryCount + 1}`);
    return new Promise(resolve => setTimeout(resolve, delay));
  }

  // RAG Data Preparation Phase: Loading, chunking, and embedding (both core docs and repo code)
  async processPushedRepo(userId, repoId, repoData) {
    // userId here is the application's internal userId (e.g., UUID from JWT)
    // repoData should now contain githubOwner (e.g., "anatolyZader")
    console.log(`[${new Date().toISOString()}] ðŸ“¥ RAG REPO: Processing repo for user ${userId}: ${repoId}`);
    console.log(`[${new Date().toISOString()}] ðŸ“¥ RAG REPO: Received repoData structure:`, JSON.stringify(repoData, null, 2)); 
    
    // Emit starting status
    this.emitRagStatus('processing_started', {
      userId,
      repoId,
      timestamp: new Date().toISOString()
    });

    // Set userId if not already set
    if (this.userId !== userId) {
      this.setUserId(userId);
    }

    // Index the core documentation (API spec, markdown files)
    await this.indexCoreDocsToPinecone();

    try {
      // Validate repoData structure
      if (!repoData || !repoData.url || !repoData.branch) {
        throw new Error(`Invalid repository data: ${JSON.stringify(repoData)}`);
      }

      const { url, branch } = repoData;

      // Extract GitHub owner and repo name from URL
      const urlParts = url.split('/');
      const githubOwner = urlParts[urlParts.length - 2];
      const repoName = urlParts[urlParts.length - 1].replace('.git', '');

      console.log(`[${new Date().toISOString()}] ðŸ“¥ RAG REPO: Extracted GitHub owner: ${githubOwner}, repo name: ${repoName}`);

      // Check if the repository is already processed
      const existingRepo = await this.findExistingRepo(userId, repoId, githubOwner, repoName);
      if (existingRepo) {
        console.log(`[${new Date().toISOString()}] Repository already processed, skipping: ${repoId}`);
        this.emitRagStatus('processing_skipped', {
          userId,
          repoId,
          reason: 'Repository already processed',
          timestamp: new Date().toISOString()
        });
        return {
          success: true,
          message: 'Repository already processed',
          repoId,
          userId
        };
      }

      // Clone the repository to a temporary location
      const tempDir = await this.cloneRepository(url, branch);

      // Load and process documents from the cloned repository
      const result = await this.loadAndProcessRepoDocuments(tempDir, userId, repoId, githubOwner, repoName);

      // Cleanup: remove the cloned repository files
      await this.cleanupTempDir(tempDir);

      return result;
    } catch (error) {
      console.error(`[${new Date().toISOString()}] âŒ Error processing repository ${repoId}:`, error.message);
      
      // Emit error status
      this.emitRagStatus('processing_error', {
        userId,
        repoId,
        error: error.message,
        phase: 'repository_processing',
        processedAt: new Date().toISOString()
      });
      
      return {
        success: false,
        error: `Repository processing failed: ${error.message}`,
        userId: userId,
        repoId: repoId,
        processedAt: new Date().toISOString()
      };
    }
  }

  // Helper method to format conversation history into messages
  formatConversationHistory(history) {
    if (!history || history.length === 0) {
      return [];
    }

    const messages = [];
    
    // Add conversation history as alternating user/assistant messages
    history.forEach((entry) => {
      // Add user message
      messages.push({
        role: "user",
        content: entry.prompt
      });
      
      // Add assistant response
      messages.push({
        role: "assistant", 
        content: entry.response
      });
    });

    console.log(`[${new Date().toISOString()}] ðŸ” CONVERSATION HISTORY: Formatted ${messages.length} messages (${messages.length / 2} exchanges) for context`);
    return messages;
  }

  // 2. Retrieval and generation:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
  // The actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes it to the model

  // Helper to extract endpoints and tags from OpenAPI spec
  formatApiSpecSummary(apiSpec) {
    if (!apiSpec) return '';
    let summary = 'API Endpoints and Tags from OpenAPI spec:';
    // List tags
    if (Array.isArray(apiSpec.tags)) {
      summary += '\n\nTags:';
      apiSpec.tags.forEach(tag => {
        summary += `\n- ${tag.name}: ${tag.description}`;
      });
    }
    // List endpoints
    if (apiSpec.paths && typeof apiSpec.paths === 'object') {
      summary += '\n\nEndpoints:';
      Object.entries(apiSpec.paths).forEach(([path, methods]) => {
        Object.entries(methods).forEach(([method, details]) => {
          const tagList = details.tags && details.tags.length ? ` [tags: ${details.tags.join(', ')}]` : '';
          summary += `\n- ${method.toUpperCase()} ${path}${tagList}`;
        });
      });
    }
    return summary;
  }

  // RAG Query Phase: Retrieval and Generation only (data preparation is done in processPushedRepo/initializeCoreDocumentation)
  async respondToPrompt(userId, conversationId, prompt, conversationHistory = []) {
    this.setUserId(userId);
    if (!this.userId) {
      console.warn(`[${new Date().toISOString()}] Failed to set userId in respondToPrompt. Provided userId: ${userId}`);
      return {
        success: false,
        response: "I'm having trouble identifying your session. Please try again in a moment.",
        conversationId: conversationId,
        timestamp: new Date().toISOString()
      };
    }

    console.log(`[${new Date().toISOString()}] Processing AI request for conversation ${conversationId}`);

    // Use the queue system for all AI operations
    return this.queueRequest(async () => {
      try {
        // Load API spec and format summary
        const apiSpec = await this.loadApiSpec('httpApiSpec.json');
        const apiSpecSummary = this.formatApiSpecSummary(apiSpec);
        
        // Build context: code chunks + API spec summary
        let contextIntro = '';
        if (typeof apiSpecSummary === 'string') contextIntro += apiSpecSummary + '\n\n';

        // If no vectorStore or pinecone client, fall back to standard mode
        if (!this.vectorStore || !this.pinecone) {
          console.warn(`[${new Date().toISOString()}] ðŸ” RAG DEBUG: Vector database not available, falling back to standard model response.`);
          
          // Use our standardized emitRagStatus method instead
          this.emitRagStatus('retrieval_disabled', {
            userId: this.userId,
            conversationId: conversationId,
            reason: 'Vector database not available'
          });
          console.log(`[${new Date().toISOString()}] [DEBUG] respondToPrompt called with userId=${userId}, conversationId=${conversationId}, prompt=${prompt}`);
          
          return await this.generateStandardResponse(prompt, conversationId, conversationHistory);
        }

        console.log(`[${new Date().toISOString()}] ðŸ” RAG DEBUG: Searching vector database for relevant code chunks for user ${this.userId}...`);
        const pineconeIndex = process.env.PINECONE_INDEX_NAME || 'eventstorm-index';
        const vectorStoreNamespace = this.userId;
        
        // Store these for later use
        this.pineconeIndex = pineconeIndex;
    console.log(`[${new Date().toISOString()}] [DEBUG] userId confirmed in respondToPrompt: ${this.userId}`);
        this.vectorStoreNamespace = vectorStoreNamespace;
        
    console.log(`[${new Date().toISOString()}] [DEBUG] queueRequest will be called for respondToPrompt.`);
        console.log(`[${new Date().toISOString()}] ðŸ” RAG DEBUG: Vector store namespace: ${vectorStoreNamespace}`);
        console.log(`[${new Date().toISOString()}] ðŸ” RAG DEBUG: Pinecone index name: ${pineconeIndex}`);

        // Attempt retrieval with queue-based approach
        let similarDocuments = [];
        
        // Check if vectorStore is available before attempting search
        if (!this.vectorStore) {
          console.log(`[${new Date().toISOString()}] ðŸ” RAG DEBUG: Vector store not available, falling back to standard response`);
          this.emitRagStatus('retrieval_disabled', {
            userId: this.userId,
            conversationId: conversationId,
            reason: 'Vector store not initialized'
          });
          return await this.generateStandardResponse(prompt, conversationId, conversationHistory);
        }
        
        try {
          // Find relevant documents from vector database with timeout
          console.log(`[${new Date().toISOString()}] ðŸ” RAG DEBUG: Running intelligent similarity search with semantic filtering`);
          
          // Determine search strategy based on prompt analysis
          const searchStrategy = this.determineSearchStrategy(prompt);
          console.log(`[${new Date().toISOString()}] ðŸ§  SEARCH STRATEGY: User=${searchStrategy.userResults} docs, Core=${searchStrategy.coreResults} docs`);
          console.log(`[${new Date().toISOString()}] ðŸ§  SEARCH FILTERS: User=${JSON.stringify(searchStrategy.userFilters)}, Core=${JSON.stringify(searchStrategy.coreFilters)}`);
          
          // Add timeout to prevent hanging - increased timeout and fallback strategy
          const VECTOR_SEARCH_TIMEOUT = 30000; // 30 seconds (increased from 10)
          
          // Search user-specific namespace with intelligent filtering
          const userSearchPromise = this.vectorStore.similaritySearch(
            prompt, 
            searchStrategy.userResults,
            searchStrategy.userFilters && Object.keys(searchStrategy.userFilters).length > 0 ? 
              { filter: searchStrategy.userFilters } : 
              {}
          );

          // Search global 'core-docs' namespace with intelligent filtering
          const coreDocsVectorStore = new PineconeStore(this.embeddings, {
              pineconeIndex: this.pinecone.Index(pineconeIndex),
              namespace: 'core-docs'
          });
          const coreDocsSearchPromise = coreDocsVectorStore.similaritySearch(
            prompt, 
            searchStrategy.coreResults,
            searchStrategy.coreFilters && Object.keys(searchStrategy.coreFilters).length > 0 ? 
              { filter: searchStrategy.coreFilters } : 
              {}
          );
          
          const timeoutPromise = new Promise((_, reject) => {
            setTimeout(() => reject(new Error('Vector search timeout')), VECTOR_SEARCH_TIMEOUT);
          });
          
          try {
            // Retrieve more chunks for richer context
            const [userDocs, coreDocs] = await Promise.race([
                Promise.all([userSearchPromise, coreDocsSearchPromise]),
                timeoutPromise
            ]);
            similarDocuments = [...userDocs, ...coreDocs];

          } catch (timeoutError) {
            console.log(`[${new Date().toISOString()}] âš ï¸ Vector search timed out, falling back to standard response`);
            this.emitRagStatus('retrieval_timeout_fallback', {
              userId: this.userId,
              conversationId: conversationId,
              error: timeoutError.message,
              query: prompt.substring(0, 100)
            });
            return await this.generateStandardResponse(prompt, conversationId, conversationHistory);
          }
          
          // Log the firstt few chunks for debugging
          similarDocuments.forEach((doc, i) => {
            console.log(`[DEBUG] Chunk ${i}: ${doc.metadata.source || 'Unknown'} | ${doc.pageContent.substring(0, 200)}`);
          });
          console.log(`[${new Date().toISOString()}] ðŸ” RAG DEBUG: Retrieved ${similarDocuments.length} documents from vector store`);
          
          console.log(`[${new Date().toISOString()}] [DEBUG] Emitted retrieval_disabled status.`);
          if (similarDocuments.length > 0) {
            console.log(`[${new Date().toISOString()}] ðŸ” RAG DEBUG: First document metadata:`, 
              JSON.stringify(similarDocuments[0].metadata, null, 2));
            console.log(`[${new Date().toISOString()}] ðŸ” RAG DEBUG: First document content preview: ${similarDocuments[0].pageContent.substring(0, 100)}...`);
            
            // Log all document sources for better debugging
        console.log(`[${new Date().toISOString()}] [DEBUG] Pinecone index: ${pineconeIndex}, VectorStore namespace: ${vectorStoreNamespace}`);
            console.log(`[${new Date().toISOString()}] ðŸ” RAG DEBUG: All retrieved document sources:`);
            similarDocuments.forEach((doc, index) => {
              console.log(`[${new Date().toISOString()}]   ${index + 1}. ${doc.metadata.source || 'Unknown'} (${doc.pageContent.length} chars)`);
            });
        console.log(`[${new Date().toISOString()}] [DEBUG] Set pineconeIndex and vectorStoreNamespace properties.`);
            
            // Use our standardized emitRagStatus method
            this.emitRagStatus('retrieval_success', {
              userId: this.userId,
              conversationId: conversationId,
              documentsFound: similarDocuments.length,
              sources: similarDocuments.map(doc => doc.metadata.source || 'Unknown'),
              sourceTypes: {
                apiSpec: similarDocuments.filter(doc => doc.metadata.type === 'apiSpec' || doc.metadata.type === 'apiSpecFull').length,
                githubCode: similarDocuments.filter(doc => doc.metadata.repoId || doc.metadata.githubOwner).length
              },
              firstDocContentPreview: similarDocuments[0].pageContent.substring(0, 100) + '...'
            });
            console.log(`[${new Date().toISOString()}] [DEBUG] similaritySearch called with user: ${userId}, prompt: ${prompt}`);
          }
          console.log(`[${new Date().toISOString()}] [DEBUG] similaritySearch returned ${similarDocuments.length} documents.`);
        } catch (error) {
          // For errors, log and continue with standard response
          console.error(`[${new Date().toISOString()}] ðŸ” RAG DEBUG: Vector search error:`, error.message);
          
            console.log(`[${new Date().toISOString()}] [DEBUG] Logging all retrieved document sources:`);
          // Use our standardized emitRagStatus method
          this.emitRagStatus('retrieval_error', {
            userId: this.userId,
            conversationId: conversationId,
            error: error.message,
            query: prompt.substring(0, 100) // Include a preview of the query that failed
          });
          console.log(`[${new Date().toISOString()}] [DEBUG] Emitted retrieval_error status.`);
          
          console.log(`[${new Date().toISOString()}] [DEBUG] similaritySearch error stack:`, error.stack);
          return await this.generateStandardResponse(prompt, conversationId, conversationHistory);
        }

        if (similarDocuments.length === 0) {
          console.log(`[${new Date().toISOString()}] ðŸ” RAG DEBUG: No relevant documents found, using standard response`);
          
          // Use our standardized emitRagStatus method
          console.log(`[${new Date().toISOString()}] [DEBUG] Emitted retrieval_error status.`);
          this.emitRagStatus('retrieval_no_results', {
            userId: this.userId,
            conversationId: conversationId,
            query: prompt.substring(0, 100) // Include a preview of the query
          });
          console.log(`[${new Date().toISOString()}] [DEBUG] No relevant documents found in similaritySearch.`);
          
          return await this.generateStandardResponse(prompt, conversationId, conversationHistory);
        }

        // Process the retrieved documents
        console.log(`[${new Date().toISOString()}] ðŸ” RAG DEBUG: Found ${similarDocuments.length} relevant documents`);

        // Note: API spec and markdown docs are now pre-indexed in core-docs namespace
        // and retrieved via similarity search above, no need to load them again here

        // Log comprehensive source loading summary
        console.log(`[${new Date().toISOString()}] ðŸ“š MULTI-SOURCE RAG CONTEXT FROM VECTOR SEARCH:`);
        console.log(`[${new Date().toISOString()}] ðŸŽ¯ TOTAL CONTEXT: ${similarDocuments.length} chunks retrieved from vector database ready for AI processing`);

        // Format the context from retrieved documents (includes pre-indexed API spec, markdown docs, and repo code)
        console.log(`[${new Date().toISOString()}] [DEBUG] Formatting context from retrieved documents.`);
        
        // Analyze and log source composition
        const sourceAnalysis = {
          apiSpec: 0,
          rootDocumentation: 0,
          moduleDocumentation: 0,
          githubRepo: 0,
          total: similarDocuments.length
        };
        
        similarDocuments.forEach(doc => {
          const type = doc.metadata.type || 'unknown';
          if (type === 'apiSpec' || type === 'apiSpecFull') sourceAnalysis.apiSpec++;
          else if (type === 'root_documentation') sourceAnalysis.rootDocumentation++;
          else if (type === 'module_documentation') sourceAnalysis.moduleDocumentation++;
          else if (doc.metadata.repoId || doc.metadata.githubOwner) sourceAnalysis.githubRepo++;
        });
        
        console.log(`[${new Date().toISOString()}] ðŸ” RAG SOURCES ANALYSIS: Chat answer will use comprehensive context from multiple sources:`);
        console.log(`[${new Date().toISOString()}] ðŸŒ API Specification: ${sourceAnalysis.apiSpec} chunks`);
        console.log(`[${new Date().toISOString()}] ðŸ“‹ Root Documentation (plugins/core): ${sourceAnalysis.rootDocumentation} chunks`);
        console.log(`[${new Date().toISOString()}] ðŸ“ Module Documentation: ${sourceAnalysis.moduleDocumentation} chunks`);
        console.log(`[${new Date().toISOString()}] ðŸ’» GitHub Repository Code: ${sourceAnalysis.githubRepo} chunks`);
        console.log(`[${new Date().toISOString()}] ðŸ“Š TOTAL CONTEXT SOURCES: ${sourceAnalysis.total} chunks from ${Object.values(sourceAnalysis).filter(v => v > 0).length - 1} different source types`);
        
        // Log specific module documentation being used
        const moduleDocsUsed = similarDocuments
          .filter(doc => doc.metadata.type === 'module_documentation')
          .map(doc => doc.metadata.module)
          .filter((module, index, arr) => arr.indexOf(module) === index);
        
        if (moduleDocsUsed.length > 0) {
          console.log(`[${new Date().toISOString()}] ðŸ“ Module docs included: ${moduleDocsUsed.join(', ')}`);
        }
        
        // Log GitHub repositories being used
        const reposUsed = similarDocuments
          .filter(doc => doc.metadata.repoId)
          .map(doc => `${doc.metadata.githubOwner}/${doc.metadata.repoId}`)
          .filter((repo, index, arr) => arr.indexOf(repo) === index);
        
        if (reposUsed.length > 0) {
          console.log(`[${new Date().toISOString()}] ðŸ’» GitHub repos referenced: ${reposUsed.join(', ')}`);
        }
        
        const context = similarDocuments.map((doc, index) => {
          const source = doc.metadata.source || 'Unknown source';
          const type = doc.metadata.type || 'unknown';
          
          // Add section headers for different types of documentation
          let sectionHeader = '';
          if (type === 'apiSpec' || type === 'apiSpecFull') {
            sectionHeader = '=== API SPECIFICATION ===\n';
          } else if (type === 'root_documentation') {
            sectionHeader = '=== ROOT DOCUMENTATION (Plugins & Core Files) ===\n';
          } else if (type === 'module_documentation') {
            sectionHeader = `=== ${doc.metadata.module?.toUpperCase() || 'MODULE'} DOCUMENTATION ===\n`;
          } else if (doc.metadata.repoId) {
            sectionHeader = `=== CODE REPOSITORY (${source}) ===\n`;
          }
          
          // Limit content length but provide more for documentation files
          const maxLength = type.includes('documentation') ? 1000 : 500;
          const content = doc.pageContent.length > maxLength 
            ? doc.pageContent.substring(0, maxLength) + '...' 
            : doc.pageContent;
            
          return `${sectionHeader}File: ${source}\n${content}`;
        }).join('\n\n');
        
        console.log(`[${new Date().toISOString()}] ðŸ” RAG DEBUG: Created context with ${context.length} characters from ${similarDocuments.length} documents`);
        console.log(`[${new Date().toISOString()}] [DEBUG] Context formatted. Length: ${context.length}`);
        
        // Format passed conversation history for context continuity
        const historyMessages = this.formatConversationHistory(conversationHistory);
        
        // Analyze context sources for intelligent prompt selection
        const contextSources = {
          apiSpec: sourceAnalysis.apiSpec > 0,
          rootDocumentation: sourceAnalysis.rootDocumentation > 0,
          moduleDocumentation: sourceAnalysis.moduleDocumentation > 0,
          code: sourceAnalysis.githubRepo > 0
        };

        // Select appropriate system prompt based on context and question
        const systemPrompt = PromptSelector.selectPrompt({
          hasRagContext: similarDocuments.length > 0,
          conversationCount: conversationHistory.length,
          question: prompt,
          contextSources: contextSources,
          mode: 'auto' // Can be overridden for testing: 'rag', 'standard', 'code', 'api', 'general'
        });

        // Determine if this is a general knowledge question using PromptConfig
        const questionLower = prompt.toLowerCase();
        
        // Check for application-specific terms
        const hasApplicationKeywords = PromptConfig.keywords.application.some(keyword => questionLower.includes(keyword));
        
        // Check for general question patterns
        const hasGeneralPattern = PromptConfig.keywords.general.some(keyword => questionLower.includes(keyword));
        
        // Special check for app-specific mentions
        const mentionsApp = questionLower.includes('eventstorm') || 
                          questionLower.includes('this app') || 
                          questionLower.includes('the app') ||
                          questionLower.includes('your app') ||
                          questionLower.includes('my app');
        
        // Logic: It's a general question ONLY if it has general patterns AND no application context
        const isGeneralQuestion = hasGeneralPattern && !hasApplicationKeywords && !mentionsApp;

        if (PromptConfig.logging.logPromptSelection) {
          console.log(`[${new Date().toISOString()}] ðŸŽ¯ PROMPT SELECTION: Auto-selected intelligent system prompt based on context analysis`);
          console.log(`[${new Date().toISOString()}] ðŸŽ¯ QUESTION TYPE: ${isGeneralQuestion ? 'General Knowledge' : 'Application/Technical'}`);
          console.log(`[${new Date().toISOString()}] ðŸ” Analysis: hasGeneral=${hasGeneralPattern}, hasApp=${hasApplicationKeywords}, mentionsApp=${mentionsApp}`);
        }
        
        // Build messages with appropriate content based on question type
        let userMessage;
        if (isGeneralQuestion) {
          // For general questions, don't include application context
          userMessage = {
            role: "user",
            content: prompt
          };
          console.log(`[${new Date().toISOString()}] ðŸ” GENERAL QUESTION: Using clean prompt without application context`);
        } else {
          // For application/technical questions, include relevant context
          userMessage = {
            role: "user",
            content: `I have a question: "${prompt}"\n\nHere is the relevant information:\n\n${context}`
          };
          console.log(`[${new Date().toISOString()}] ðŸ” APPLICATION QUESTION: Including RAG context for technical assistance`);
        }
        
        // Build comprehensive messages array with intelligent system prompt, history, and appropriate content
        const messages = [
          {
            role: "system",
            content: systemPrompt
          },
          ...historyMessages, // Include conversation history
          userMessage
        ];
        
        console.log(`[${new Date().toISOString()}] ðŸ” CONVERSATION CONTEXT: Built ${messages.length} messages for LLM (1 system + ${historyMessages.length} history + 1 current)`);
        console.log(`[${new Date().toISOString()}] [DEBUG] Messages prepared for LLM invoke with conversation history.`);
        
        // Try to generate a response
        let retries = 0;
        let success = false;
        let response;
        while (!success && retries < this.maxRetries) {
          if (await this.checkRateLimit()) {
            console.log(`[${new Date().toISOString()}] [DEBUG] Starting LLM invoke loop. MaxRetries: ${this.maxRetries}`);
            try {
              const result = await this.llm.invoke(messages);
              response = result.content;
              success = true;
              console.log(`[${new Date().toISOString()}] [DEBUG] LLM invoke successful.`);
            } catch (error) {
              if (error.message && (error.message.includes('429') || error.message.includes('quota') || error.message.includes('rate limit'))) {
                retries++;
                console.warn(`[${new Date().toISOString()}] Rate limit hit during generation, retry ${retries}/${this.maxRetries}`);
                await this.waitWithBackoff(retries);
                console.log(`[${new Date().toISOString()}] [DEBUG] LLM rate limit encountered. Waiting before retry.`);
              } else {
                // Log the error and throw it for proper handling
                console.error(`[${new Date().toISOString()}] Failed to respond to prompt:`, error);
                console.log(`[${new Date().toISOString()}] [DEBUG] LLM invoke error stack:`, error.stack);
                throw error;
              }
            }
          } else {
            // Wait if we're rate limited
            await this.waitWithBackoff(retries);
            console.log(`[${new Date().toISOString()}] [DEBUG] Waiting for rate limit window in LLM invoke.`);
          }
        }
        if (!success) {
          // If we couldn't generate a response after all retries
          throw new Error(`Failed to generate response after ${this.maxRetries} retries due to rate limits`);
        }
        console.log(`[${new Date().toISOString()}] Successfully generated response for conversation ${conversationId}`);
        
        // Log comprehensive source usage summary
        console.log(`[${new Date().toISOString()}] âœ… CHAT RESPONSE GENERATED using MULTI-SOURCE RAG:`);
        console.log(`[${new Date().toISOString()}] ðŸ“Š Sources Used Summary:`);
        console.log(`[${new Date().toISOString()}]    â€¢ API spec chunks: ${sourceAnalysis.apiSpec}`);
        console.log(`[${new Date().toISOString()}]    â€¢ Root docs chunks: ${sourceAnalysis.rootDocumentation}`);
        console.log(`[${new Date().toISOString()}]    â€¢ Module docs chunks: ${sourceAnalysis.moduleDocumentation}`);
        console.log(`[${new Date().toISOString()}]    â€¢ GitHub code chunks: ${sourceAnalysis.githubRepo}`);
        console.log(`[${new Date().toISOString()}]    â€¢ TOTAL: ${sourceAnalysis.total} chunks`);
        
        if (moduleDocsUsed.length > 0) {
          console.log(`[${new Date().toISOString()}] ðŸ“ Modules referenced: ${moduleDocsUsed.join(', ')}`);
        }
        if (reposUsed.length > 0) {
          console.log(`[${new Date().toISOString()}] ðŸ’» Repositories referenced: ${reposUsed.join(', ')}`);
        }
        
        const sourcesBreakdown = {
          hasApiSpec: sourceAnalysis.apiSpec > 0,
          hasRootDocs: sourceAnalysis.rootDocumentation > 0,
          hasModuleDocs: sourceAnalysis.moduleDocumentation > 0,
          hasGithubCode: sourceAnalysis.githubRepo > 0
        };
        
        console.log(`[${new Date().toISOString()}] ðŸŽ¯ COMPREHENSIVE CONTEXT: Answer incorporates ${Object.values(sourcesBreakdown).filter(Boolean).length}/4 available source types`);

        // Log the full context size to help diagnose if embedding usage is working
        const contextSize = (typeof finalContext !== 'undefined' ? finalContext.length : (typeof context !== 'undefined' ? context.length : contextIntro.length));
        console.log(`[${new Date().toISOString()}] [DEBUG] Response generated. Context size: ${context.length}`);
        console.log(`[${new Date().toISOString()}] ðŸ” CONVERSATION HISTORY: Used ${conversationHistory.length} previous exchanges for continuity`);
        console.log(`[${new Date().toISOString()}] [DEBUG] Returning response object from respondToPrompt.`);
        
        return {
          success: true,
          response,
          conversationId,
          timestamp: new Date().toISOString(),
          ragEnabled: true,
          contextSize,
          sourcesUsed: sourceAnalysis,
          sourcesBreakdown,
          conversationHistoryUsed: conversationHistory.length > 0,
          historyMessages: conversationHistory.length
        };
      } catch (error) {
        console.error(`[${new Date().toISOString()}] Error in respondToPrompt:`, error.message);
        return {
          success: false,
          response: "I encountered an issue while processing your request. Please try again shortly.",
          conversationId,
          timestamp: new Date().toISOString(),
          error: error.message
        };
      }
    });
  }

  // Helper method for generating responses without context
  async generateStandardResponse(prompt, conversationId, conversationHistory = []) {
    try {
      // Format passed conversation history for continuity even in standard responses
      const historyMessages = this.formatConversationHistory(conversationHistory);
      
      // Use intelligent prompt selection even for standard responses
      const systemPrompt = PromptSelector.selectPrompt({
        hasRagContext: false,
        conversationCount: conversationHistory.length,
        question: prompt,
        contextSources: {},
        mode: 'auto'
      });

      if (PromptConfig.logging.logPromptSelection) {
        console.log(`[${new Date().toISOString()}] ðŸŽ¯ STANDARD RESPONSE: Selected intelligent prompt for non-RAG response`);
      }
      
      // Build messages with intelligent conversation history
      const messages = [
        {
          role: "system",
          content: systemPrompt
        },
        ...historyMessages, // Include conversation history
        {
          role: "user",
          content: prompt
        }
      ];

      console.log(`[${new Date().toISOString()}] ðŸ” STANDARD RESPONSE: Built ${messages.length} messages with conversation history (1 system + ${historyMessages.length} history + 1 current)`);

      // Rate limit checks
      let retries = 0;
      let success = false;
      let response;

      while (!success && retries < this.maxRetries) {
        if (await this.checkRateLimit()) {
          try {
            const result = await this.llm.invoke(messages);
            response = result.content;
            success = true;
          } catch (error) {
            if (error.message && (error.message.includes('429') || error.message.includes('quota') || error.message.includes('rate limit'))) {
              retries++;
              console.warn(`[${new Date().toISOString()}] Rate limit hit during standard generation, retry ${retries}/${this.maxRetries}`);
              await this.waitWithBackoff(retries);
            } else {
              throw error;
            }
          }
        } else {
          await this.waitWithBackoff(retries);
        }
      }

      if (!success) {
        throw new Error(`Failed to generate standard response after ${this.maxRetries} retries due to rate limits`);
      }

      console.log(`[${new Date().toISOString()}] ðŸ” RAG DEBUG: Generated standard response with conversation history for conversation ${conversationId}`);
      
      return {
        success: true,
        response,
        conversationId,
        timestamp: new Date().toISOString(),
        sourcesUsed: 0,
        ragEnabled: false,
        conversationHistoryUsed: conversationHistory.length > 0,
        historyMessages: conversationHistory.length
      };

    } catch (error) {
      console.error(`[${new Date().toISOString()}] Error in generateStandardResponse:`, error.message);

      if (error.message && (error.message.includes('429') || error.message.includes('quota') || error.message.includes('rate limit'))) {
        return {
          success: false,
          response: "I'm currently experiencing high demand. Please try again in a few moments.",
          conversationId,
          timestamp: new Date().toISOString(),
          error: error.message
        };
      }

      return {
        success: false,
        response: "I encountered an issue while generating a response. Please try again shortly.",
        conversationId,
        timestamp: new Date().toISOString(),
        error: error.message
      };
    }
  }

  // Helper method to determine file type from file path
  getFileType(filePath) {
    const extension = filePath.split('.').pop().toLowerCase();
    const codeExtensions = {
      js: 'JavaScript',
      jsx: 'React',
      ts: 'TypeScript',
      tsx: 'React TypeScript',
      py: 'Python',
      java: 'Java',
      rb: 'Ruby',
      php: 'PHP',
      c: 'C',
      cpp: 'C++',
      cs: 'C#',
      go: 'Go',
      rs: 'Rust',
      swift: 'Swift',
      kt: 'Kotlin',
      html: 'HTML',
      css: 'CSS',
      scss: 'SCSS',
      json: 'JSON',
      md: 'Markdown',
      sql: 'SQL',
      sh: 'Shell',
      bat: 'Batch',
      ps1: 'PowerShell',
      yaml: 'YAML',
      yml: 'YAML',
      xml: 'XML'
    };

    return codeExtensions[extension] || 'Unknown';
  }

  // Helper method to sanitize document IDs
  sanitizeId(input) {
    // Remove special characters and truncate if necessary
    return input.replace(/[^a-zA-Z0-9_-]/g, '_').substring(0, 50);
  }

  // Helper method to emit RAG status updates for monitoring
  emitRagStatus(status, details = {}) {
    // Always log the status update
    console.log(`[${new Date().toISOString()}] ðŸ” RAG STATUS: ${status}`, 
      Object.keys(details).length > 0 ? JSON.stringify(details, null, 2) : '');
    
    // Try to emit to the event bus if available
    try {
      // First try the instance event bus
      if (this.eventBus) {
        this.eventBus.emit('ragStatusUpdate', {
          component: 'aiLangchainAdapter',
          timestamp: new Date().toISOString(),
          status,
          ...details
        });
        return;
      }
      
      // Fallback to imported event bus if instance one isn't available
      const eventDispatcherPath = '../../../../eventDispatcher';
      const { eventBus } = require(eventDispatcherPath);
      if (eventBus) {
        eventBus.emit('ragStatusUpdate', {
          component: 'aiLangchainAdapter',
          timestamp: new Date().toISOString(),
          status,
          ...details
        });
      }
    } catch (error) {
      console.warn(`[${new Date().toISOString()}] âš ï¸ Failed to emit RAG status update: ${error.message}`);
    }
  }

  // Helper method to create an appropriate text splitter based on document content
  createSmartSplitter(documents) {
    // Default splitter settings
    const chunkSize = 1500; // Larger chunks for fewer requests
    const chunkOverlap = 250; // More overlap for better context

    // Analyze documents to determine predominant language and file types
    const languageCount = {};
    const fileTypeCount = {};

    documents.forEach(doc => {
      const source = doc.metadata.source || '';
      const extension = source.split('.').pop().toLowerCase();
      if (!extension) return;

      // Track file types
      if (['md', 'markdown'].includes(extension)) {
        fileTypeCount.markdown = (fileTypeCount.markdown || 0) + 1;
      } else if (['json'].includes(extension)) {
        fileTypeCount.json = (fileTypeCount.json || 0) + 1;
      } else if (['yml', 'yaml'].includes(extension)) {
        fileTypeCount.yaml = (fileTypeCount.yaml || 0) + 1;
      }

      // Track programming languages
      if (['js', 'jsx', 'ts', 'tsx'].includes(extension)) languageCount.javascript = (languageCount.javascript || 0) + 1;
      else if (['py'].includes(extension)) languageCount.python = (languageCount.python || 0) + 1;
      else if (['java'].includes(extension)) languageCount.java = (languageCount.java || 0) + 1;
      else if (['rb'].includes(extension)) languageCount.ruby = (languageCount.ruby || 0) + 1;
      else if (['go'].includes(extension)) languageCount.golang = (languageCount.golang || 0) + 1;
      else if (['php'].includes(extension)) languageCount.php = (languageCount.php || 0) + 1;
      else if (['c', 'cpp', 'h', 'hpp'].includes(extension)) languageCount.cpp = (languageCount.cpp || 0) + 1;
      else if (['cs'].includes(extension)) languageCount.csharp = (languageCount.csharp || 0) + 1;
      else if (['rs'].includes(extension)) languageCount.rust = (languageCount.rust || 0) + 1;
    });

    // Check for predominant file types
    const totalDocs = documents.length;
    const markdownRatio = (fileTypeCount.markdown || 0) / totalDocs;

    // If majority are markdown files, use header-based splitting
    if (markdownRatio > 0.5) {
      console.log(`[${new Date().toISOString()}] Using markdown header-based splitter (${Math.round(markdownRatio * 100)}% markdown files)`);
      return new MarkdownHeaderTextSplitter({
        headersToSplitOn: [
          { level: 1, name: 'h1' },
          { level: 2, name: 'h2' },
          { level: 3, name: 'h3' }
        ]
      });
    }

    // Find the most common programming language
    let predominantLanguage = 'javascript'; // Default
    let maxCount = 0;

    Object.entries(languageCount).forEach(([lang, count]) => {
      if (count > maxCount) {
        predominantLanguage = lang;
        maxCount = count;
      }
    });

    // Map to LangChain language string
    const langMap = {
      javascript: "js",
      python: "python",
      java: "java",
      ruby: "ruby",
      golang: "go",
      php: "php",
      cpp: "cpp",
      csharp: "csharp",
      rust: "rust"
    };

    const language = langMap[predominantLanguage] || "js";

    console.log(`[${new Date().toISOString()}] Using ${predominantLanguage} code splitter for document processing (${maxCount}/${totalDocs} files)`);

    // Create a language-specific splitter that respects code structure
    return RecursiveCharacterTextSplitter.fromLanguage(language, {
      chunkSize,
      chunkOverlap,
      // Add separators that respect code structure
      separators: this.getCodeAwareSeparators(language)
    });
  }

  // Get code-aware separators for better chunking
  getCodeAwareSeparators(language) {
    const baseSeparators = [
      "\n\n", // Paragraph breaks
      "\n", // Line breaks
      " ", // Word breaks
      "" // Character breaks (fallback)
    ];

    switch (language) {
      case "js":
      case "ts":
        return [
          "\nexport class ", // Class declarations
          "\nclass ", // Class declarations
          "\nexport function ", // Function exports
          "\nfunction ", // Function declarations
          "\nexport const ", // Constant exports
          "\nconst ", // Constants
          "\nexport ", // Any exports
          "\n// ", // Comments
          "\n\n", // Paragraph breaks
          "\n", // Line breaks
          " ", // Word breaks
          ""
        ];
      case "python":
        return [
          "\nclass ", // Class declarations
          "\ndef ", // Function definitions
          "\n# ", // Comments
          "\n\n", // Paragraph breaks
          "\n", // Line breaks
          " ", // Word breaks
          ""
        ];
      case "java":
        return [
          "\npublic class ", // Public classes
          "\nclass ", // Classes
          "\npublic ", // Public methods/fields
          "\nprivate ", // Private methods/fields
          "\n// ", // Comments
          "\n\n", // Paragraph breaks
          "\n", // Line breaks
          " ", // Word breaks
          ""
        ];
      default:
        return baseSeparators;
    }
  }

  // Queue system for rate limiting
  startQueueProcessor() {
    // Process queue every 5 seconds
    setInterval(() => this.processQueue(), 5000);
    console.log(`[${new Date().toISOString()}] AI request queue processor started`);
  }

  async processQueue() {
    if (this.isProcessingQueue || this.requestQueue.length === 0) return;

    this.isProcessingQueue = true;
    console.log(`[${new Date().toISOString()}] Processing AI request queue, ${this.requestQueue.length} items pending`);

    try {
      // Get the next request from the queue
      const request = this.requestQueue.shift();

      // Check if we're within rate limits
      if (await this.checkRateLimit()) {
        console.log(`[${new Date().toISOString()}] Processing queued request: ${request.id}`);

        try {
          // Execute the request
          const result = await request.execute();

          // Resolve the promise with the result
          request.resolve(result);
        } catch (error) {
          // If the request fails, reject the promise
          request.reject(error);
        }
      } else {
        // If we're rate limited, put the request back at the front of the queue
        console.log(`[${new Date().toISOString()}] Rate limited, requeueing request: ${request.id}`);
        this.requestQueue.unshift(request);

        // Wait before processing more
        await new Promise(resolve => setTimeout(resolve, 5000));
      }
    } finally {
      this.isProcessingQueue = false;

      // If there are more items in the queue, process them
      if (this.requestQueue.length > 0) {
        setTimeout(() => this.processQueue(), 1000);
      }
    }
  }

  // Add a request to the queue
  queueRequest(execute) {
    return new Promise((resolve, reject) => {
      const requestId = Math.random().toString(36).substring(2, 10);

      this.requestQueue.push({
        id: requestId,
        execute,
        resolve,
        reject,
        timestamp: new Date().toISOString()
      });

      console.log(`[${new Date().toISOString()}] Added request ${requestId} to queue, queue length: ${this.requestQueue.length}`);

      // Trigger queue processing if not already running
      if (!this.isProcessingQueue) {
        setTimeout(() => this.processQueue(), 100);
      }
    });
  }

  /**
   * Determine intelligent search strategy based on prompt analysis
   */
  determineSearchStrategy(prompt) {
    const promptLower = prompt.toLowerCase();
    
    // Domain/business logic questions
    if (this.containsKeywords(promptLower, ['domain', 'entity', 'business', 'rule', 'aggregate', 'model'])) {
      console.log(`[${new Date().toISOString()}] ðŸ§  SEARCH STRATEGY: Domain/Business Logic Query`);
      return {
        userResults: 8,
        coreResults: 3,
        userFilters: { layer: 'domain' },
        coreFilters: { type: 'module_documentation' }
      };
    }
    
    // API/endpoint questions
    if (this.containsKeywords(promptLower, ['api', 'endpoint', 'route', 'http', 'request', 'controller', 'fastify'])) {
      console.log(`[${new Date().toISOString()}] ðŸ§  SEARCH STRATEGY: API/Endpoint Query`);
      return {
        userResults: 6,
        coreResults: 5,
        userFilters: { semantic_role: 'controller' },
        coreFilters: { type: 'api_endpoint' }
      };
    }
    
    // Error/debugging questions
    if (this.containsKeywords(promptLower, ['error', 'bug', 'fix', 'debug', 'issue', 'problem', 'exception', 'fail'])) {
      console.log(`[${new Date().toISOString()}] ðŸ§  SEARCH STRATEGY: Error/Debugging Query`);
      return {
        userResults: 10,
        coreResults: 2,
        userFilters: { is_entrypoint: true }, // Focus on entry points where errors often occur
        coreFilters: {}
      };
    }
    
    // Chat/conversation questions
    if (this.containsKeywords(promptLower, ['chat', 'message', 'conversation', 'websocket', 'socket'])) {
      console.log(`[${new Date().toISOString()}] ðŸ§  SEARCH STRATEGY: Chat Module Query`);
      return {
        userResults: 8,
        coreResults: 3,
        userFilters: { eventstorm_module: 'chatModule' },
        coreFilters: { type: 'module_documentation' }
      };
    }
    
    // Git/repository questions
    if (this.containsKeywords(promptLower, ['git', 'repository', 'github', 'pull request', 'commit', 'branch'])) {
      console.log(`[${new Date().toISOString()}] ðŸ§  SEARCH STRATEGY: Git Module Query`);
      return {
        userResults: 8,
        coreResults: 3,
        userFilters: { eventstorm_module: 'gitModule' },
        coreFilters: { type: 'module_documentation' }
      };
    }
    
    // AI/RAG/embedding questions
    if (this.containsKeywords(promptLower, ['ai', 'embedding', 'vector', 'rag', 'langchain', 'openai', 'semantic'])) {
      console.log(`[${new Date().toISOString()}] ðŸ§  SEARCH STRATEGY: AI Module Query`);
      return {
        userResults: 8,
        coreResults: 3,
        userFilters: { eventstorm_module: 'aiModule' },
        coreFilters: { type: 'module_documentation' }
      };
    }
    
    // Wiki/documentation questions
    if (this.containsKeywords(promptLower, ['wiki', 'documentation', 'search', 'knowledge', 'doc'])) {
      console.log(`[${new Date().toISOString()}] ðŸ§  SEARCH STRATEGY: Wiki Module Query`);
      return {
        userResults: 8,
        coreResults: 4,
        userFilters: { eventstorm_module: 'wikiModule' },
        coreFilters: { type: 'module_documentation' }
      };
    }
    
    // Test/testing questions
    if (this.containsKeywords(promptLower, ['test', 'testing', 'spec', 'unit test', 'integration test'])) {
      console.log(`[${new Date().toISOString()}] ðŸ§  SEARCH STRATEGY: Testing Query`);
      return {
        userResults: 8,
        coreResults: 2,
        userFilters: { semantic_role: 'test' },
        coreFilters: {}
      };
    }
    
    // Configuration/setup questions
    if (this.containsKeywords(promptLower, ['config', 'configuration', 'setup', 'environment', 'env', 'settings'])) {
      console.log(`[${new Date().toISOString()}] ðŸ§  SEARCH STRATEGY: Configuration Query`);
      return {
        userResults: 6,
        coreResults: 4,
        userFilters: { semantic_role: 'config' },
        coreFilters: { type: 'configuration' }
      };
    }
    
    // Plugin/middleware questions
    if (this.containsKeywords(promptLower, ['plugin', 'middleware', 'interceptor', 'fastify plugin'])) {
      console.log(`[${new Date().toISOString()}] ðŸ§  SEARCH STRATEGY: Plugin/Middleware Query`);
      return {
        userResults: 8,
        coreResults: 3,
        userFilters: { semantic_role: 'plugin' },
        coreFilters: {}
      };
    }
    
    // Default strategy for general questions
    console.log(`[${new Date().toISOString()}] ðŸ§  SEARCH STRATEGY: General Query (default)`);
    return {
      userResults: 8,
      coreResults: 4,
      userFilters: {},
      coreFilters: {}
    };
  }

  /**
   * Check if prompt contains specific keywords
   */
  containsKeywords(text, keywords) {
    return keywords.some(keyword => text.includes(keyword));
  }
}

module.exports = AILangchainAdapter;